{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A notebook for exploring video posedata\n",
    "\n",
    "**Intended use:** the user selects a video that is accompanied by already extracted posedata in a .json file. The notebook provides visualizations that summarize the quality and content of the poses extracted across all frames of the video, as well as armature plots of the detected poses in a selected frame. These can be viewed separately from the source video and even animated.\n",
    "\n",
    "Note that at present, this only works with .json output files generated via the Open PifPaf command-line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import (\n",
    "    Button,\n",
    "    CrosshairTool,\n",
    "    DatetimeTickFormatter,\n",
    "    Div,\n",
    "    LinearAxis,\n",
    "    Range1d,\n",
    "    Slider,\n",
    "    Span,\n",
    "    TapTool,\n",
    "    Toggle,\n",
    ")\n",
    "from bokeh.models.widgets.inputs import Select\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.themes import Theme\n",
    "import cv2\n",
    "from IPython.display import HTML, display\n",
    "from ipywidgets import Dropdown, Layout\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and display the video/posedata selector widget\n",
    "\n",
    "**Important:** for a video to appear in the dropdown menu, the video and its posedata output file must be present at the path specified in `source_data_folder`, which is by default the folder containing this notebook. The names of the matched video and posedata files should be identical, other than that the posedata file will have .openpifpaf.json appended to the name of the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to specify a different source data folder, do it like this:\n",
    "source_data_folder = Path(\"/Users/peterbroadwell/Documents/mime/\")\n",
    "# source_data_folder = Path(os.getenv(\"DATA_FOLDER\", Path.cwd()))\n",
    "\n",
    "\n",
    "def get_available_videos(data_folder):\n",
    "    \"\"\"\n",
    "    Available videos will be limited to those with a .json and matching video (.mp4, .avi, etc)\n",
    "    file in a predefined directory (the notebook's running directory, for now)\n",
    "    \"\"\"\n",
    "    available_json_files = list(data_folder.glob(\"*.json\"))\n",
    "    available_video_files = (\n",
    "        p.resolve()\n",
    "        for p in Path(data_folder).glob(\"*\")\n",
    "        if p.suffix in {\".avi\", \".mp4\", \".mov\", \".mkv\", \".webm\"}\n",
    "    )\n",
    "    available_json = [\n",
    "        json_file.stem.split(\".\")[0] for json_file in available_json_files\n",
    "    ]\n",
    "\n",
    "    available_videos = []\n",
    "\n",
    "    for video_name in available_video_files:\n",
    "        if video_name.stem.split(\".\")[0] in available_json:\n",
    "            available_videos.append(video_name.name)\n",
    "\n",
    "    return available_videos\n",
    "\n",
    "\n",
    "select_msg = (\n",
    "    \"<style>.widget-label { min-width: 20ex !important; }</style>\"\n",
    "    \"<body><p>Please select the video to explore from the dropdown list. To be available in the list, \"\n",
    "    \"the pose estimation output .json file and the original video file must share the same name, except \"\n",
    "    f\"for the .openpifaf.json extension, and be stored in {source_data_folder}/.</p></body>\"\n",
    ")\n",
    "\n",
    "display(HTML(select_msg))\n",
    "\n",
    "video_selector = Dropdown(\n",
    "    options=get_available_videos(source_data_folder),\n",
    "    description=\"Video to explore:\",\n",
    "    disabled=False,\n",
    "    layout=Layout(width=\"60%\", height=\"40px\"),\n",
    ")\n",
    "\n",
    "video_selector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect video and per-frame pose metadata for the selected video\n",
    "The video should be selected from the drop-down menu above after running the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = source_data_folder / video_selector.value\n",
    "\n",
    "pose_file = f\"{video_file}.openpifpaf.json\"\n",
    "\n",
    "print(\"Video file:\", video_file)\n",
    "print(\"Posedata file:\", pose_file)\n",
    "\n",
    "cap = cv2.VideoCapture(str(video_file))\n",
    "video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(\"Video FPS:\", video_fps)\n",
    "\n",
    "print(\"Processing video and JSON files, please wait...\")\n",
    "\n",
    "pose_json = jsonlines.open(pose_file)\n",
    "pose_data = []\n",
    "\n",
    "# Per-frame pose data: frame, seconds, num_poses, avg_pose_conf, avg_coords_per_pose\n",
    "pose_series = {\n",
    "    \"frame\": [],\n",
    "    \"seconds\": [],\n",
    "    \"timestamp\": [],\n",
    "    \"num_poses\": [],\n",
    "    \"avg_score\": [],\n",
    "    \"avg_coords_per_pose\": [],\n",
    "}\n",
    "\n",
    "for frame in pose_json:\n",
    "\n",
    "    pose_data.append(frame)\n",
    "\n",
    "    # Frame output is numbered from 1 in the JSON\n",
    "    seconds = float(frame[\"frame\"] - 1) / video_fps\n",
    "\n",
    "    num_poses = len(frame[\"predictions\"])\n",
    "    pose_series[\"num_poses\"].append(num_poses)\n",
    "\n",
    "    pose_series[\"frame\"].append(frame[\"frame\"] - 1)\n",
    "    pose_series[\"seconds\"].append(seconds)\n",
    "\n",
    "    # Construct a timestamp that can be used with Bokeh's DatetimeTickFormatter\n",
    "    td = timedelta(seconds=seconds)\n",
    "    datestring = str(td)\n",
    "    if td.microseconds == 0:\n",
    "        datestring += \".000000\"\n",
    "    dt = datetime.strptime(datestring, \"%H:%M:%S.%f\")\n",
    "\n",
    "    pose_series[\"timestamp\"].append(dt)\n",
    "\n",
    "    pose_scores = []\n",
    "    pose_coords_counts = []\n",
    "    avg_score = 0  # NaN for empty frames?\n",
    "    avg_coords_per_pose = 0\n",
    "\n",
    "    normalized_poses = []\n",
    "\n",
    "    for pose in frame[\"predictions\"]:\n",
    "\n",
    "        # ??? Do something with the bbox? The avg ratio of bbox area to full screen can indicate closeup\n",
    "        # vs. long shot (could also run monoloco to get this kind of info + more)\n",
    "\n",
    "        pose_scores.append(pose[\"score\"])\n",
    "        pose_coords = 0\n",
    "        for i in range(0, len(pose[\"keypoints\"]), 3):\n",
    "            # Mostly ignore the coord confidence value, unless it's 0 (coord not found)\n",
    "            # These seem to be averaged into the full pose \"score\" already\n",
    "            if pose[\"keypoints\"][i + 2] != 0:\n",
    "                pose_coords += 1\n",
    "\n",
    "        # To find the typically small proportion of poses that are complete\n",
    "        # if pose_coords == 17:\n",
    "        #     print(frame['frame'])\n",
    "\n",
    "        pose_coords_counts.append(pose_coords)\n",
    "\n",
    "    if num_poses > 0:\n",
    "        avg_score = sum(pose_scores) / num_poses\n",
    "        avg_coords_per_pose = sum(pose_coords_counts) / num_poses\n",
    "\n",
    "    pose_series[\"avg_score\"].append(avg_score)\n",
    "    pose_series[\"avg_coords_per_pose\"].append(avg_coords_per_pose)\n",
    "\n",
    "print(\"Total frames:\", len(pose_series[\"frame\"]))\n",
    "\n",
    "print(\"Duration:\", pose_series[\"timestamp\"][len(pose_series[\"timestamp\"]) - 1].time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default dimension of single pose viz\n",
    "POSE_MAX_DIM = 100\n",
    "\n",
    "\n",
    "def unflatten_pose_data(prediction):\n",
    "    \"\"\"\n",
    "    Convert an Open PifPaf pose prediction (a 1D 51-element list) into a 17-element\n",
    "    list (not a NumPy array) of [x_coord, y_coord, confidence] triples.\n",
    "    \"\"\"\n",
    "    return np.array_split(prediction[\"keypoints\"], len(prediction[\"keypoints\"]) / 3)\n",
    "\n",
    "\n",
    "def extract_trustworthy_coords(prediction):\n",
    "    \"\"\"\n",
    "    Perform the often-desired task of converting an Open PifPaf pose prediction\n",
    "    from a 1D vector of coordinates and confidence values to a 17x2 NumPy array\n",
    "    containing only the armature coordinates, with coordinate values set to NaN,NaN\n",
    "    for any coordinate with a confidence value of 0.\n",
    "    \"\"\"\n",
    "    unflattened_pose = unflatten_pose_data(prediction)\n",
    "    return np.array([[coords[0], coords[1]] if coords[2] != 0 else [np.NaN, np.NaN] for coords in unflattened_pose]).flatten()\n",
    "\n",
    "\n",
    "def get_pose_extent(prediction):\n",
    "    \"\"\"Get the min and max x and y coordinates of an Open PifPaf pose prediction\"\"\"\n",
    "    pose_coords = unflatten_pose_data(prediction)\n",
    "    min_x = np.NaN\n",
    "    min_y = np.NaN\n",
    "    max_x = np.NaN\n",
    "    max_y = np.NaN\n",
    "    for coords in pose_coords:\n",
    "        # Coordinates with confidence values of 0 are not considered\n",
    "        if coords[2] == 0:\n",
    "            continue\n",
    "        min_x = np.nanmin([min_x, coords[0]])\n",
    "        min_y = np.nanmin([min_y, coords[1]])\n",
    "        max_x = np.nanmax([max_x, coords[0]])\n",
    "        max_y = np.nanmax([max_y, coords[1]])\n",
    "\n",
    "    return [min_x, min_y, max_x, max_y]\n",
    "\n",
    "\n",
    "def shift_pose_to_origin(prediction):\n",
    "    \"\"\"\n",
    "    Shift the keypoint coordinates of an Open PifPaf pose prediction so that the\n",
    "    min x and y coordinates of its extent are at the 0,0 origin.\n",
    "    NOTE: This only returns the modified 'keypoints' portion of the prediction.\n",
    "    \"\"\"\n",
    "    pose_coords = unflatten_pose_data(prediction)\n",
    "    min_x, min_y, max_x, max_y = get_pose_extent(prediction)\n",
    "\n",
    "    for i, coords in enumerate(pose_coords):\n",
    "        # Coordinates with confidence values of 0 are not modified; these should not\n",
    "        # be used in any pose representations or calculations, and often (but not\n",
    "        # always) already have 0,0 coordinates.\n",
    "        if coords[2] == 0:\n",
    "            continue\n",
    "        pose_coords[i] = [coords[0] - min_x, coords[1] - min_y, coords[2]]\n",
    "\n",
    "    return {\"keypoints\": np.concatenate(pose_coords, axis=None)}\n",
    "\n",
    "\n",
    "def rescale_pose_coords(prediction):\n",
    "    \"\"\"\n",
    "    Rescale the coordinates of an OpenPifPaf pose prediction so that the extent\n",
    "    of the pose's long axis is equal to the global POSE_MAX_DIM setting. The\n",
    "    coordinates of the short axis are scaled by the same factor, and then are\n",
    "    shifted so that the short axis is centered within the POSE_MAX_DIM extent.\n",
    "    NOTE: This only returns the modified 'keypoints' portion of the prediction.\n",
    "    \"\"\"\n",
    "    pose_coords = unflatten_pose_data(prediction)\n",
    "    min_x, min_y, max_x, max_y = get_pose_extent(prediction)\n",
    "\n",
    "    scale_factor = POSE_MAX_DIM / np.max([max_x, max_y])\n",
    "\n",
    "    x_extent = max_x - min_x\n",
    "    y_extent = max_y - min_y\n",
    "\n",
    "    if x_extent >= y_extent:\n",
    "        x_recenter = 0\n",
    "        y_recenter = round((POSE_MAX_DIM - (scale_factor * y_extent)) / 2)\n",
    "    else:\n",
    "        x_recenter = round((POSE_MAX_DIM - (scale_factor * x_extent)) / 2)\n",
    "        y_recenter = 0\n",
    "\n",
    "    for i, coords in enumerate(pose_coords):\n",
    "        # Coordinates with confidence values of 0 are not modified; these should not\n",
    "        # be used in any pose representations or calculations, and often (but not\n",
    "        # always) already have 0,0 coordinates.\n",
    "        if coords[2] == 0:\n",
    "            continue\n",
    "        pose_coords[i] = [\n",
    "            round(coords[0] * scale_factor + x_recenter),\n",
    "            round(coords[1] * scale_factor + y_recenter),\n",
    "            coords[2],\n",
    "        ]\n",
    "\n",
    "    return {\"keypoints\": np.concatenate(pose_coords, axis=None)}\n",
    "\n",
    "\n",
    "def shift_normalize_rescale_pose_coords(prediction):\n",
    "    \"\"\"\n",
    "    Convenience function to shift an Open PifPaf pose prediction so that its minimal corner\n",
    "    is at the origin, then rescale so that it fits into a POSE_MAX_DIM * POSE_MAX_DIM extent.\n",
    "    NOTE: This only returns the modified 'keypoints' portion of the prediction.\n",
    "    \"\"\"\n",
    "    return rescale_pose_coords(shift_pose_to_origin(prediction))\n",
    "\n",
    "\n",
    "def compare_poses_cosine(p1, p2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity of the 'keypoint' portions of two Open PifPaf pose predictions\n",
    "    by computing their cosine distance and subtracting this from 1 (so 1=identical).\n",
    "    \"\"\"\n",
    "    unflattened_p1 = unflatten_pose_data(p1)\n",
    "    return 1 - cosine(\n",
    "        np.array(unflatten_pose_data(p1))[:, :2].flatten(),\n",
    "        np.array(unflatten_pose_data(p2))[:, :2].flatten(),\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_joint_angles(prediction):\n",
    "    \"\"\"\n",
    "    Build an additional/alternative feature set for an Open PifPaf pose prediction, composed\n",
    "    of the angles, measured in radians, of several joints/articulation points on the body (see\n",
    "    list in code comments below).\n",
    "    \"\"\"\n",
    "    pose_coords = unflatten_pose_data(prediction)\n",
    "\n",
    "    joint_angles = []\n",
    "\n",
    "    # Joints to use:\n",
    "    joint_angle_points = [\n",
    "        [3, 5, 11],  # Left ear - left shoulder - left hip\n",
    "        [4, 6, 12],  # Right ear - right shoulder - right hip\n",
    "        [11, 5, 7],  # Left hip - left shoulder - left elbow\n",
    "        [12, 6, 8],  # Right hip - right shoulder - right elbow\n",
    "        [5, 7, 9],  # Left shoulder - left elbow - left wrist\n",
    "        [6, 8, 10],  # Right shoulder - right elbow - right wrist\n",
    "        [5, 11, 13],  # Left shoulder - left hip - left knee\n",
    "        [6, 12, 14],  # Right shoulder - right hip - right knee\n",
    "        [11, 13, 15],  # Left hip - left knee - left ankle\n",
    "        [12, 14, 16],  # Right hip - right knee - right ankle\n",
    "    ]\n",
    "\n",
    "    for angle_points in joint_angle_points:\n",
    "        # Need 3 points to make an angle; if 1 or more are missing, it's a NaN\n",
    "        if (\n",
    "            pose_coords[angle_points[0]][2] == 0\n",
    "            or pose_coords[angle_points[1]][2] == 0\n",
    "            or pose_coords[angle_points[2]][2] == 0\n",
    "        ):\n",
    "            joint_angles.append(np.NaN)\n",
    "        else:\n",
    "            ba = np.array(\n",
    "                [pose_coords[angle_points[0]][0], pose_coords[angle_points[0]][1]]\n",
    "            ) - np.array(\n",
    "                [pose_coords[angle_points[1]][0], pose_coords[angle_points[1]][1]]\n",
    "            )\n",
    "            bc = np.array(\n",
    "                [pose_coords[angle_points[2]][0], pose_coords[angle_points[2]][1]]\n",
    "            ) - np.array(\n",
    "                [pose_coords[angle_points[1]][0], pose_coords[angle_points[1]][1]]\n",
    "            )\n",
    "            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "            joint_angles.append(np.arccos(cosine_angle))\n",
    "\n",
    "    return joint_angles\n",
    "\n",
    "\n",
    "def compare_poses_angles(joint_angles1, joint_angles2):\n",
    "    \"\"\"\n",
    "    This computes a similarity score for two pose predictions that are represented\n",
    "    as vectors of joint angles. The similarity metric is essentially standard cosine\n",
    "    similarity (that tge values in the vectors are angle measurements does not make\n",
    "    a difference to how it works; they're just treated as numbers), modified to handle\n",
    "    missing/NaN vector values gracefully. (1=identical)\n",
    "    \"\"\"\n",
    "\n",
    "    angles_dot = np.nansum(np.array(joint_angles1) * np.array(joint_angles2))\n",
    "    angles_norm = np.sqrt(np.nansum(np.square(np.array(joint_angles1)))) * np.sqrt(\n",
    "        np.nansum(np.square(np.array(joint_angles2)))\n",
    "    )\n",
    "    return angles_dot / angles_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(value, max=100):\n",
    "    return HTML(\n",
    "        \"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(\n",
    "            value=value, max=max\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Computing normalized poses for comparison and clustering\")\n",
    "bar = display(progress_bar(0, len(pose_data)), display_id=True)\n",
    "\n",
    "# For cluster analysis, each pose must be a 1D array, and all poses must be in a 1D list\n",
    "# that includes only the pose keypoint coordinates (not the confidence scores).\n",
    "# So we also create a parallel data structure to keep track of the frame number and numbering\n",
    "# within the frame of each of the poses.\n",
    "normalized_pose_data = []\n",
    "normalized_pose_metadata = []\n",
    "\n",
    "for i, frame in enumerate(pose_data):\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        bar.update(progress_bar(i, len(pose_data)))\n",
    "\n",
    "    for j, pose in enumerate(frame[\"predictions\"]):\n",
    "        normalized_coords = extract_trustworthy_coords(shift_normalize_rescale_pose_coords(pose))\n",
    "        #normalized_pose = unflatten_pose_data(shift_normalize_rescale_pose_coords(pose))\n",
    "        #normalized_coords = np.array([[coords[0], coords[1]] if coords[2] != 0 else [np.NaN, np.NaN] for coords in normalized_pose]).flatten()\n",
    "        normalized_pose_data.append(normalized_coords)\n",
    "        normalized_pose_metadata.append({\"frameno\": i, \"poseno\": j})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS indexes of the poses, for fast nearest neighbor similarity search\n",
    "\n",
    "import faiss\n",
    "\n",
    "# FAISS can't handle NaNs in the iput vectors so use -1s instead\n",
    "faiss_pose_data = [tuple(np.nan_to_num(raw_pose, nan=-1).tolist()) for raw_pose in normalized_pose_data]\n",
    "\n",
    "# This builds an exact (flat) index based on Euclidean distance\n",
    "faiss_L2_index = faiss.IndexFlatL2(34)\n",
    "faiss_L2_input = np.array(faiss_pose_data).astype('float32')\n",
    "faiss_L2_index.add(faiss_L2_input)\n",
    "\n",
    "# This builds an exact (flat) index based on inner-product distance,\n",
    "# which is equivalent to cosine similarity when the inputs are normalized\n",
    "faiss_IP_index = faiss.IndexFlatIP(34)\n",
    "faiss_IP_input = np.array(faiss_pose_data).astype('float32')\n",
    "faiss.normalize_L2(faiss_IP_input) # Must normalize the inputs!\n",
    "faiss_IP_index.add(faiss_IP_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a really long time. XXX Try FAISS's k-means\n",
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "MIN_SAMPLES_PER_CLUSTER = 50\n",
    "\n",
    "cluster_labels = OPTICS(\n",
    "    min_samples=MIN_SAMPLES_PER_CLUSTER, metric=\"sqeuclidean\"\n",
    ").fit_predict(normalized_pose_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing poses for numerical comparison, clustering\n",
    "\n",
    "Assumptions:\n",
    "- Representations derived from different armature-based pose estimation libraries can differ in the number of points and/or angles. One could provide pose \"translators\" that interpolate or extrapolate the locations of missing points in order to allow comparison of poses from different systems. It's not obvious how often this would need to be done, however, so implementing it might be more trouble than it's worth.\n",
    "- The core representation logics will be agnostic to the size of the figure in the frame (or even in the environment, if we're lucky enough to be able to determine that); distances will be normalized or eschewed in favor of angles. Size can however still be included as an optional parameter, e.g., the size of the figure's bounding box in pixels.\n",
    "- Methods will default to 2D, but can accommodate 3D input values by adding further coordinate dimensions or a second angle.\n",
    "- 3D representations can include an angle indicating the direction of the \"front\" of the pose. It's possible to do something like this for 2D representations, but rectifying 2D coordinates to a front-on representation may be too noisy and thus not worth the effort. Unless otherwise indicated, 2D representations are always from the camera's point of view.\n",
    "- *Missing input coordinates* leading to missing features in pose representations: if not interpolated/estimated, these should be represented as NaNs, not 0s, to avoid confusion with legitimate points at the 0,0 origin, or 0-degree angles. The comparison methods may need to support at least two primary modes for dealing with these: a) only poses containing the same features may be compared; all others produce a null result, or b) poses containing different features may be compared, but poses that share a greater number of features should generally count as more similar than poses that share a smaller number of features.\n",
    "\n",
    "Primary techniques for representing poses:\n",
    "1. Normalized coordinates (or distances between points). \n",
    "1. Angles of three-point armatures (elbows, shoulders, knees, hips)\n",
    "1. Directions of movement and magnitudes of movement of normalized points \n",
    "\n",
    "Note that #3 can be combined constructively with #1 or #2 (turning a pose into a \"movelet\"), but combining #1 and #2 would be redundant in almost all cases. Obtaining sufficiently accurate data to infer instantaneous directions and magnitude of movement for specific points is not always possible, however.\n",
    "\n",
    "Pose comparison best practices:\n",
    "- Convert each pose to a vector, then compute the similarity between them.\n",
    "- If poses are incomplete, only compare the available data points; points missing from one or both poses are considered null/equal (all relevant distance metrics require this anyway).\n",
    "- Normalization to ensure scale-invariance of comparisons (considering only 2D coords so far): absolute size, aka distance from camera, shouldn't affect comparison. The most promising method at present involves scaling the largest dimension so that it fits into a set range (e.g., 500 pixels), then scaling the smaller dimension by the same factor. The pose therefore will stretch across the full extent of the larger dimension, and is centered around the middle of the set range of the smaller dimension, which should aid in vector-based comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and launch the explorer app\n",
    "\n",
    "This displays an interactive chart visualization of the attributes of the posedata in the .json output file across the runtime of the video.\n",
    "\n",
    "Clicking anywhere in the chart, moving the slider, or clicking the prev/next buttons will select a frame and draw the poses detected in that frame, with the option of displaying the actual image from the source video as the \"background.\"\n",
    "\n",
    "Please see the cell below the next if you are running this notebook in VS Code. Note also that the Jupyter server must be running on port 8888 for the explorer app to work in Jupyter/JupterLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The body part numberings and armature connectors for the 17-keypoint COCO pose format are defined in\n",
    "# https://github.com/openpifpaf/openpifpaf/blob/main/src/openpifpaf/plugins/coco/constants.py\n",
    "# Note that the body part numbers in the connector (skeleton) definitions begin with 1, for some reason, not 0\n",
    "OPP_COCO_SKELETON = [\n",
    "    (16, 14),\n",
    "    (14, 12),\n",
    "    (17, 15),\n",
    "    (15, 13),\n",
    "    (12, 13),\n",
    "    (6, 12),\n",
    "    (7, 13),\n",
    "    (6, 7),\n",
    "    (6, 8),\n",
    "    (7, 9),\n",
    "    (8, 10),\n",
    "    (9, 11),\n",
    "    (2, 3),\n",
    "    (1, 2),\n",
    "    (1, 3),\n",
    "    (2, 4),\n",
    "    (3, 5),\n",
    "    (4, 6),\n",
    "    (5, 7),\n",
    "]\n",
    "OPP_COCO_COLORS = [\n",
    "    \"orangered\",\n",
    "    \"orange\",\n",
    "    \"blue\",\n",
    "    \"lightblue\",\n",
    "    \"darkgreen\",\n",
    "    \"red\",\n",
    "    \"lightgreen\",\n",
    "    \"pink\",\n",
    "    \"plum\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"saddlebrown\",\n",
    "    \"mediumorchid\",\n",
    "    \"gray\",\n",
    "    \"salmon\",\n",
    "    \"chartreuse\",\n",
    "    \"lightgray\",\n",
    "    \"darkturquoise\",\n",
    "    \"goldenrod\",\n",
    "]\n",
    "\n",
    "UPSCALE = 3  # See draw_frame()\n",
    "\n",
    "# Default dimensions of the output visualizations\n",
    "FIGURE_WIDTH = 950\n",
    "FIGURE_HEIGHT = 500\n",
    "\n",
    "# XXX ImageDraw does't ship with a scaleable font, so best to use matplotlib's\n",
    "import matplotlib\n",
    "\n",
    "font_path = os.path.join(\n",
    "    matplotlib.__path__[0], \"mpl-data\", \"fonts\", \"ttf\", \"DejaVuSans.ttf\"\n",
    ")\n",
    "try:\n",
    "    label_font = ImageFont.truetype(font_path, size=128)\n",
    "except:\n",
    "    label_font = None\n",
    "\n",
    "\n",
    "def add_pose_to_drawing(pose_prediction, drawing, seqno=None, show_bbox=False):\n",
    "    pose_coords = unflatten_pose_data(pose_prediction)\n",
    "\n",
    "    for i, seg in enumerate(OPP_COCO_SKELETON):\n",
    "\n",
    "        if pose_coords[seg[0] - 1][2] == 0 or pose_coords[seg[1] - 1][2] == 0:\n",
    "            continue\n",
    "\n",
    "        line_color = OPP_COCO_COLORS[i]\n",
    "        shape = [\n",
    "            (\n",
    "                int(pose_coords[seg[0] - 1][0] * UPSCALE),\n",
    "                int(pose_coords[seg[0] - 1][1] * UPSCALE),\n",
    "            ),\n",
    "            (\n",
    "                int(pose_coords[seg[1] - 1][0] * UPSCALE),\n",
    "                int(pose_coords[seg[1] - 1][1]) * UPSCALE,\n",
    "            ),\n",
    "        ]\n",
    "        drawing.line(shape, fill=line_color, width=2 * UPSCALE)\n",
    "\n",
    "    if \"bbox\" in pose_prediction:\n",
    "        bbox = pose_prediction[\"bbox\"]\n",
    "    else:\n",
    "        extent = get_pose_extent(pose_prediction)\n",
    "        bbox = [extent[0], extent[1], extent[2] - extent[0], extent[3] - extent[1]]\n",
    "\n",
    "    # bbox format for PifPaf is x0, y0, width, height\n",
    "    # Also note that both PifPaf and PIL/ImageDraw place (0,0) at top left, not bottom left\n",
    "    upper_left = (int(bbox[0] * UPSCALE), int(bbox[1] * UPSCALE))\n",
    "    lower_right = (\n",
    "        int((bbox[0] + bbox[2]) * UPSCALE),\n",
    "        int((bbox[1] + bbox[3]) * UPSCALE),\n",
    "    )\n",
    "\n",
    "    if show_bbox:\n",
    "        shape = [upper_left, lower_right]\n",
    "        drawing.rectangle(shape, outline=\"blue\", width=1 * UPSCALE)\n",
    "\n",
    "    if seqno is not None:\n",
    "        drawing.text(\n",
    "            upper_left, str(seqno + 1), font=label_font, align=\"right\", fill=\"blue\"\n",
    "        )\n",
    "\n",
    "    return drawing\n",
    "\n",
    "\n",
    "def normalize_and_draw_pose(pose_prediction):\n",
    "    normalized_prediction = shift_normalize_rescale_pose_coords(pose_prediction)\n",
    "    # XXX Could also grab the background image and excerpt/scale it to match, if desired...\n",
    "    bg_img = Image.new(\"RGBA\", (POSE_MAX_DIM * UPSCALE, POSE_MAX_DIM * UPSCALE))\n",
    "    drawing = ImageDraw.Draw(bg_img)\n",
    "    drawing = add_pose_to_drawing(normalized_prediction, drawing)\n",
    "    bg_img = bg_img.resize(\n",
    "        (POSE_MAX_DIM, POSE_MAX_DIM), resample=Image.Resampling.LANCZOS\n",
    "    )\n",
    "    return bg_img\n",
    "\n",
    "\n",
    "def image_from_video_frame(video_file, frameno):\n",
    "    \"\"\"Grabs the specified frame from the video and converts it into an RGBA array\"\"\"\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    cap.set(1, frameno)\n",
    "    ret, img = cap.read()\n",
    "    rgba_img = cv2.cvtColor(img, cv2.COLOR_RGB2RGBA)\n",
    "    image = np.asarray(rgba_img)\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_frame(frame, bg_img=None):\n",
    "    \"\"\"Draws the poses in the specified frame, superimposing them on the frame image, if provided.\"\"\"\n",
    "\n",
    "    pixels_to_poses = {}\n",
    "\n",
    "    # The only way to get smooth(er) lines in the pose armatures via PIL ImageDraw is to upscale the entire\n",
    "    # image by some factor, draw the lines, then downscale back to the original resolution while applying\n",
    "    # Lanczos resampling, because ImageDraw doesn't do any native anti-aliasing.\n",
    "    if bg_img is None:\n",
    "        bg_img = Image.new(\"RGBA\", (video_width * UPSCALE, video_height * UPSCALE))\n",
    "    else:\n",
    "        bg_img = bg_img.resize((video_width * UPSCALE, video_height * UPSCALE))\n",
    "\n",
    "    drawing = ImageDraw.Draw(bg_img)\n",
    "\n",
    "    for i, pose_prediction in enumerate(frame[\"predictions\"]):\n",
    "\n",
    "        drawing = add_pose_to_drawing(pose_prediction, drawing, i, show_bbox=True)\n",
    "\n",
    "    bg_img = bg_img.resize(\n",
    "        (video_width, video_height), resample=Image.Resampling.LANCZOS\n",
    "    )\n",
    "\n",
    "    return bg_img\n",
    "\n",
    "\n",
    "def pil_to_bokeh_image(pil_img, target_width, target_height):\n",
    "    \"\"\"The Bokeh interactive notebook tools will only display image data if it's formatted in a particular way\"\"\"\n",
    "    img_array = np.array(pil_img.transpose(Image.Transpose.FLIP_TOP_BOTTOM))\n",
    "\n",
    "    img = np.empty(img_array.shape[:2], dtype=np.uint32)\n",
    "    view = img.view(dtype=np.uint8).reshape(img_array.shape)\n",
    "\n",
    "    for i in range(target_height):\n",
    "        for j in range(target_width):\n",
    "            view[i, j, 0] = img_array[i, j, 0]\n",
    "            view[i, j, 1] = img_array[i, j, 1]\n",
    "            view[i, j, 2] = img_array[i, j, 2]\n",
    "            view[i, j, 3] = img_array[i, j, 3]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def bkapp(doc):\n",
    "    \"\"\"Define and run the Bokeh interactive notebook (Python + Javascript) application\"\"\"\n",
    "\n",
    "    max_y = max(pose_series[\"avg_coords_per_pose\"] + pose_series[\"num_poses\"])\n",
    "\n",
    "    # This is the main interactive timeline chart\n",
    "    tl = figure(\n",
    "        width=FIGURE_WIDTH,\n",
    "        height=FIGURE_HEIGHT,\n",
    "        title=video_file.name,\n",
    "        min_border=10,\n",
    "        y_range=(0, max_y + 1),\n",
    "        tools=\"save,box_zoom,pan,reset\",\n",
    "    )\n",
    "    # Format the X axis as hour-minute-second timecodes\n",
    "    tl.x_range = Range1d(min(pose_series[\"timestamp\"]), max(pose_series[\"timestamp\"]))\n",
    "    tl.xaxis.axis_label = \"Time\"\n",
    "    time_formatter = DatetimeTickFormatter(\n",
    "        hourmin=\"%H:%M:%S\",\n",
    "        minutes=\"%H:%M:%S\",\n",
    "        minsec=\"%H:%M:%S\",\n",
    "        seconds=\"%Ss\",\n",
    "        milliseconds=\"%3Nms\",\n",
    "    )\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"num_poses\"],\n",
    "        legend_label=\"Poses per frame\",\n",
    "        color=\"blue\",\n",
    "        alpha=0.6,\n",
    "        line_width=2,\n",
    "    )\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"avg_coords_per_pose\"],\n",
    "        legend_label=\"Coords per pose\",\n",
    "        color=\"red\",\n",
    "        alpha=0.6,\n",
    "        line_width=2,\n",
    "    )\n",
    "    # The left Y axis corresponds to counts of poses and coordinates\n",
    "    tl.yaxis.axis_label = \"Poses or Coords\"\n",
    "    tl.extra_y_ranges = {\"avg_score\": Range1d(0, 1)}\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"avg_score\"],\n",
    "        y_range_name=\"avg_score\",\n",
    "        legend_label=\"Avg pose score\",\n",
    "        color=\"green\",\n",
    "        alpha=0.4,\n",
    "        line_width=2,\n",
    "    )\n",
    "    # The right Y axis corresponds to the average pose score (from 0 to 1)\n",
    "    tl.add_layout(\n",
    "        LinearAxis(y_range_name=\"avg_score\", axis_label=\"Avg Pose Score\"), \"right\"\n",
    "    )\n",
    "    tl.xaxis.formatter = time_formatter\n",
    "    tl.xaxis.ticker.desired_num_ticks = 10\n",
    "    tl.legend.click_policy = \"hide\"\n",
    "    frame_line = Span(\n",
    "        location=pose_series[\"timestamp\"][0],\n",
    "        dimension=\"height\",\n",
    "        line_color=\"red\",\n",
    "        line_width=3,\n",
    "    )\n",
    "    tl.add_layout(frame_line)\n",
    "\n",
    "    def tl_tap(event):\n",
    "        \"\"\"When the chart is clicked, move the slider to the appropriate frame\"\"\"\n",
    "        # event.x is a timestamp, so it needs to be converted to a frameno\n",
    "        start_dt = datetime(1900, 1, 1)\n",
    "        dt = datetime.utcfromtimestamp(event.x / 1000)\n",
    "        t_delta = dt - start_dt\n",
    "        clicked_frame = round(t_delta.total_seconds() * video_fps)\n",
    "        slider.value = clicked_frame\n",
    "\n",
    "    tl_tap_tool = TapTool()\n",
    "    tl_crosshair_tool = CrosshairTool()\n",
    "\n",
    "    def get_frame_info(fn):\n",
    "        return f\"Frame info: {pose_series['num_poses'][fn]} detected poses, {pose_series['avg_coords_per_pose'][fn]:.3f} avg coords/pose, {pose_series['avg_score'][fn]:.3f} avg pose score\"\n",
    "\n",
    "    info_div = Div(text=get_frame_info(0))\n",
    "\n",
    "    tl.add_tools(tl_tap_tool, tl_crosshair_tool)\n",
    "    tl.on_event(\"tap\", tl_tap)\n",
    "\n",
    "    # This is the second figure, where the poses in the selected frame are drawn\n",
    "    fr = figure(\n",
    "        x_range=(0, video_width),\n",
    "        y_range=(0, video_height),\n",
    "        width=FIGURE_WIDTH,\n",
    "        height=int(FIGURE_WIDTH / video_width * video_height),\n",
    "        title=\"Poses in selected frame\",\n",
    "        tools=\"save\",\n",
    "    )\n",
    "    # Add an invisible glyph to suppress the \"figure has no renderers\" warning\n",
    "    fr.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    pose_info_div = Div(text=\"Click to poses to compare\")\n",
    "\n",
    "    pose_p1 = figure(\n",
    "        x_range=(0, POSE_MAX_DIM),\n",
    "        y_range=(0, POSE_MAX_DIM),\n",
    "        width=POSE_MAX_DIM * 2,\n",
    "        height=POSE_MAX_DIM * 2,\n",
    "        title=\"\",\n",
    "        tools=\"\",\n",
    "    )\n",
    "    # Add an invisible glyph to suppress the \"figure has no renderers\" warning\n",
    "    pose_p1.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    pose_p2 = figure(\n",
    "        x_range=(0, POSE_MAX_DIM),\n",
    "        y_range=(0, POSE_MAX_DIM),\n",
    "        width=POSE_MAX_DIM * 2,\n",
    "        height=POSE_MAX_DIM * 2,\n",
    "        title=\"\",\n",
    "        tools=\"\",\n",
    "    )\n",
    "    # Add an invisible glyph to suppress the \"figure has no renderers\" warning\n",
    "    pose_p2.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    def background_toggle_handler(event):\n",
    "        \"\"\"When the image underlay is toggled on or off, prompt the slider to redraw the frame\"\"\"\n",
    "        slider_callback(None, slider.value, slider.value)\n",
    "\n",
    "    background_switch = Toggle(label=\"show background\", active=False)\n",
    "    background_switch.on_click(background_toggle_handler)\n",
    "\n",
    "    def slider_callback(attr, old, new):\n",
    "        \"\"\"When the slider moves, draw the poses in the new frame and show the background if desired\"\"\"\n",
    "        fr.renderers = []\n",
    "        if background_switch.active:\n",
    "            cap = cv2.VideoCapture(str(video_file))\n",
    "            cap.set(1, new)\n",
    "            ret, img = cap.read()\n",
    "            rgb_bg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            rgba_bg = cv2.cvtColor(rgb_bg, cv2.COLOR_RGB2RGBA)\n",
    "            pil_bg = Image.fromarray(rgba_bg)\n",
    "            frame_img = draw_frame(pose_data[new], pil_bg)\n",
    "        else:\n",
    "            frame_img = draw_frame(pose_data[new])\n",
    "        img = pil_to_bokeh_image(frame_img, video_width, video_height)\n",
    "        fr.image_rgba(image=[img], x=0, y=0, dw=img.shape[1], dh=img.shape[0])\n",
    "        if old != new:\n",
    "            info_div.text = get_frame_info(new)\n",
    "            frame_line.location = pose_series[\"timestamp\"][new]\n",
    "            pose_p1.title.text = \"\"\n",
    "            pose_p1.renderers = []\n",
    "            pose_p2.title.text = \"\"\n",
    "            pose_p2.renderers = []\n",
    "            pose_info_div.text = \"Click two poses to compare\"\n",
    "\n",
    "    slider = Slider(\n",
    "        start=0, end=len(pose_data) - 1, value=0, step=1, title=\"Selected frame\"\n",
    "    )\n",
    "    slider.on_change(\"value\", slider_callback)\n",
    "\n",
    "    def get_pose_extent_maps(frameno):\n",
    "        pose_extent_maps = []\n",
    "        for i, pose_prediction in enumerate(pose_data[frameno][\"predictions\"]):\n",
    "\n",
    "            if \"bbox\" in pose_prediction:\n",
    "                bbox = pose_prediction[\"bbox\"]\n",
    "            else:\n",
    "                extent = get_pose_extent(pose_prediction)\n",
    "                bbox = [\n",
    "                    extent[0],\n",
    "                    extent[1],\n",
    "                    extent[2] - extent[0],\n",
    "                    extent[3] - extent[1],\n",
    "                ]\n",
    "\n",
    "            extent_map = {\n",
    "                \"poseno\": i,\n",
    "                \"min_x\": bbox[0],\n",
    "                \"min_y\": video_height - bbox[3] - bbox[1],\n",
    "                \"max_x\": bbox[0] + bbox[2],\n",
    "                \"max_y\": video_height - bbox[1],\n",
    "            }\n",
    "\n",
    "            pose_extent_maps.append(extent_map)\n",
    "\n",
    "        return pose_extent_maps\n",
    "\n",
    "    def match_pose_pixel_maps(x, y, pose_extent_maps):\n",
    "        matched_poses = []\n",
    "        for extent_map in pose_extent_maps:\n",
    "            if (\n",
    "                x >= extent_map[\"min_x\"]\n",
    "                and x <= extent_map[\"max_x\"]\n",
    "                and y >= extent_map[\"min_y\"]\n",
    "                and y <= extent_map[\"max_y\"]\n",
    "            ):\n",
    "                matched_poses.append(extent_map[\"poseno\"])\n",
    "        return matched_poses\n",
    "\n",
    "    def fr_tap(event):\n",
    "        \"\"\"When the frame is clicked\"\"\"\n",
    "        # event.x is a timestamp, so it needs to be converted to a frameno\n",
    "        pixel_key = f\"{int(event.x)}, {int(event.y)}\"\n",
    "        pose_extent_maps = get_pose_extent_maps(slider.value)\n",
    "        clicked_poses = match_pose_pixel_maps(event.x, event.y, pose_extent_maps)\n",
    "        if len(clicked_poses):\n",
    "            pose_img = normalize_and_draw_pose(\n",
    "                pose_data[slider.value][\"predictions\"][clicked_poses[0]]\n",
    "            )\n",
    "            pose_img = pil_to_bokeh_image(pose_img, POSE_MAX_DIM, POSE_MAX_DIM)\n",
    "\n",
    "            if pose_p1.title.text == \"\":\n",
    "                pose_p1.image_rgba(\n",
    "                    image=[pose_img],\n",
    "                    x=0,\n",
    "                    y=0,\n",
    "                    dw=pose_img.shape[1],\n",
    "                    dh=pose_img.shape[0],\n",
    "                )\n",
    "                pose_p1.title = f\"{clicked_poses[0]+1}\"\n",
    "                pose_info_div.text = \"Please click another pose for comparison\"\n",
    "            elif pose_p1.title.text != \"\" and pose_p2.title.text == \"\":\n",
    "                pose_p2.image_rgba(\n",
    "                    image=[pose_img],\n",
    "                    x=0,\n",
    "                    y=0,\n",
    "                    dw=pose_img.shape[1],\n",
    "                    dh=pose_img.shape[0],\n",
    "                )\n",
    "                pose_p2.title = f\"{clicked_poses[0]+1}\"\n",
    "\n",
    "                normalized_p1 = shift_normalize_rescale_pose_coords(\n",
    "                    pose_data[slider.value][\"predictions\"][int(pose_p1.title.text) - 1]\n",
    "                )\n",
    "                normalized_p2 = shift_normalize_rescale_pose_coords(\n",
    "                    pose_data[slider.value][\"predictions\"][int(pose_p2.title.text) - 1]\n",
    "                )\n",
    "\n",
    "                cosine_distance = compare_poses_cosine(\n",
    "                    normalized_p1,\n",
    "                    normalized_p2,\n",
    "                )\n",
    "                p1_angles = compute_joint_angles(normalized_p1)\n",
    "                p2_angles = compute_joint_angles(normalized_p2)\n",
    "                pose_p2.title = f\"{clicked_poses[0]+1}\"\n",
    "                angle_distance = compare_poses_angles(p1_angles, p2_angles)\n",
    "                pose_info_div.text = f\"Cosine similarity between pose keypoints: {(cosine_distance*100):3.3f}% | Similarity between pose joint angles: {(angle_distance*100):3.3f}%\"\n",
    "\n",
    "    fr_tap_tool = TapTool()\n",
    "\n",
    "    fr.add_tools(fr_tap_tool)\n",
    "    fr.on_event(\"tap\", fr_tap)\n",
    "\n",
    "    def prev_handler(event):\n",
    "        slider.value = max(0, slider.value - 1)\n",
    "\n",
    "    def next_handler(event):\n",
    "        slider.value = min(slider.value + 1, len(pose_data) - 1)\n",
    "\n",
    "    prev_button = Button(label=\"prev\")\n",
    "    prev_button.on_click(prev_handler)\n",
    "    next_button = Button(label=\"next\")\n",
    "    next_button.on_click(next_handler)\n",
    "\n",
    "    L2_search_info_div = Div(text=\"L2 (Euclidean distance) similar pose search\")\n",
    "    IP_search_info_div = Div(text=\"IP (cosine distance) similar pose search\")\n",
    "\n",
    "    SIMILAR_POSES_TO_FIND = 4\n",
    "    similar_L2_poses = []\n",
    "    similar_IP_poses = []\n",
    "\n",
    "    for s in range(SIMILAR_POSES_TO_FIND):\n",
    "        similar_L2_poses.append(\n",
    "            figure(\n",
    "                x_range=(0, POSE_MAX_DIM),\n",
    "                y_range=(0, POSE_MAX_DIM),\n",
    "                width=POSE_MAX_DIM * 2,\n",
    "                height=POSE_MAX_DIM * 2,\n",
    "                title=\"\",\n",
    "                tools=\"\",\n",
    "            )\n",
    "        )\n",
    "    for pose_box in similar_L2_poses:\n",
    "        pose_box.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    for s in range(SIMILAR_POSES_TO_FIND):\n",
    "        similar_IP_poses.append(\n",
    "            figure(\n",
    "                x_range=(0, POSE_MAX_DIM),\n",
    "                y_range=(0, POSE_MAX_DIM),\n",
    "                width=POSE_MAX_DIM * 2,\n",
    "                height=POSE_MAX_DIM * 2,\n",
    "                title=\"\",\n",
    "                tools=\"\",\n",
    "            )\n",
    "        )\n",
    "    for pose_box in similar_IP_poses:\n",
    "        pose_box.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    def get_similar_poses_handler(event):\n",
    "        \"\"\"\n",
    "        NOTE that although the FAISS search should always return the query pose as the\n",
    "        first search result, we don't want to include it in the displayed results,\n",
    "        hence the +1s and -1s in the code below.\n",
    "        \"\"\"\n",
    "        if pose_p1.title.text == \"\":\n",
    "            return\n",
    "\n",
    "        for pose_box in similar_L2_poses:\n",
    "            pose_box.renderers = []\n",
    "        for pose_box in similar_IP_poses:\n",
    "            pose_box.renderers = []\n",
    "\n",
    "        target_frameno = slider.value\n",
    "        target_poseno = pose_p1.title.text\n",
    "\n",
    "        target_pose = extract_trustworthy_coords(shift_normalize_rescale_pose_coords(\n",
    "            pose_data[slider.value][\"predictions\"][int(pose_p1.title.text) - 1]\n",
    "        ))\n",
    "        target_L2_pose_query = np.array([np.nan_to_num(target_pose, nan=-1)]).astype(\"float32\")\n",
    "        target_IP_pose_query = np.array([target_pose]).astype(\"float32\")\n",
    "\n",
    "        L2_D, L2_I = faiss_L2_index.search(target_L2_pose_query, SIMILAR_POSES_TO_FIND+1)\n",
    "        IP_D, IP_I = faiss_IP_index.search(target_L2_pose_query, SIMILAR_POSES_TO_FIND+1)\n",
    "\n",
    "        L2_match_framenos = []\n",
    "        L2_match_scores = []\n",
    "\n",
    "        IP_match_framenos = []\n",
    "        IP_match_scores = []\n",
    "\n",
    "        for m in range(1, SIMILAR_POSES_TO_FIND+1):\n",
    "            if m == 0:\n",
    "                continue\n",
    "\n",
    "            L2_match_index = L2_I[0][m]\n",
    "            IP_match_index = IP_I[0][m]\n",
    "\n",
    "            if L2_match_index != -1:\n",
    "                L2_target_frameno = normalized_pose_metadata[L2_match_index]['frameno']\n",
    "                L2_target_poseno = normalized_pose_metadata[L2_match_index]['poseno']\n",
    "                L2_match_framenos.append(str(L2_target_frameno))\n",
    "                L2_match_scores.append(str(L2_D[0][m]))\n",
    "\n",
    "                L2_match_img = normalize_and_draw_pose(\n",
    "                    pose_data[L2_target_frameno][\"predictions\"][L2_target_poseno]\n",
    "                )\n",
    "                L2_match_img = pil_to_bokeh_image(L2_match_img, POSE_MAX_DIM, POSE_MAX_DIM)\n",
    "\n",
    "                similar_L2_poses[m-1].image_rgba(\n",
    "                    image=[L2_match_img],\n",
    "                    x=0,\n",
    "                    y=0,\n",
    "                    dw=L2_match_img.shape[1],\n",
    "                    dh=L2_match_img.shape[0],\n",
    "                )\n",
    "\n",
    "            if IP_match_index != -1:\n",
    "                IP_target_frameno = normalized_pose_metadata[IP_match_index]['frameno']\n",
    "                IP_target_poseno = normalized_pose_metadata[IP_match_index]['poseno']\n",
    "                IP_match_framenos.append(str(IP_target_frameno))\n",
    "                IP_match_scores.append(str(IP_D[0][m]))\n",
    "\n",
    "                IP_match_img = normalize_and_draw_pose(\n",
    "                    pose_data[IP_target_frameno][\"predictions\"][IP_target_poseno]\n",
    "                )\n",
    "                IP_match_img = pil_to_bokeh_image(IP_match_img, POSE_MAX_DIM, POSE_MAX_DIM)\n",
    "\n",
    "                similar_IP_poses[m-1].image_rgba(\n",
    "                    image=[IP_match_img],\n",
    "                    x=0,\n",
    "                    y=0,\n",
    "                    dw=IP_match_img.shape[1],\n",
    "                    dh=IP_match_img.shape[0],\n",
    "                )\n",
    "\n",
    "        L2_search_info_div.text = f\"L2 matches in frames {', '.join(L2_match_framenos)} | scores {', '.join(L2_match_scores)}\"\n",
    "        IP_search_info_div.text = f\"IP matches in frames {', '.join(IP_match_framenos)} | scores {', '.join(IP_match_scores)}\"\n",
    "\n",
    "    def reset_subposes_handler(event):\n",
    "        pose_p1.title.text = \"\"\n",
    "        pose_p1.renderers = []\n",
    "        pose_p2.title.text = \"\"\n",
    "        pose_p2.renderers = []\n",
    "        pose_info_div.text = \"\"\n",
    "        for pose_box in similar_L2_poses:\n",
    "            pose_box.renderers = []\n",
    "        for pose_box in similar_IP_poses:\n",
    "            pose_box.renderers = []\n",
    "        L2_search_info_div.text = \"L2 (Euclidean distance) similar pose search\"\n",
    "        IP_search_info_div.text = \"IP (cosine distance) similar pose search\"\n",
    "\n",
    "    reset_subposes_button = Button(label=\"clear\")\n",
    "    reset_subposes_button.on_click(reset_subposes_handler)\n",
    "\n",
    "    get_similar_poses_button = Button(label=\"look up 1st pose\")\n",
    "    get_similar_poses_button.on_click(get_similar_poses_handler)\n",
    "\n",
    "    control_row = row(children=[prev_button, next_button, background_switch])\n",
    "\n",
    "    pose_buttons_column = column(reset_subposes_button, get_similar_poses_button)\n",
    "\n",
    "    subposes_row = row(children=[pose_p1, pose_p2, pose_buttons_column])\n",
    "\n",
    "    similar_L2_poses_row = row(children=similar_L2_poses)\n",
    "    similar_IP_poses_row = row(children=similar_IP_poses)\n",
    "\n",
    "    layout_column = column(\n",
    "        tl,\n",
    "        slider,\n",
    "        info_div,\n",
    "        control_row,\n",
    "        fr,\n",
    "        pose_info_div,\n",
    "        subposes_row,\n",
    "        L2_search_info_div,\n",
    "        similar_L2_poses_row,\n",
    "        IP_search_info_div,\n",
    "        similar_IP_poses_row,\n",
    "    )\n",
    "\n",
    "    doc.add_root(layout_column)\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "show(bkapp, notebook_url=\"localhost:8889\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the notebook in VS Code:** As of late 2022, if you are running this notebook in VS Code instead of Jupyter or JupyterLab, the above cell will not work (BokehJS will load, but no figures will appear) without the following workaround:\n",
    "\n",
    "Take note of the error message that appears when you try to run the cell above, particularly the long alphanumeric string suggested as a value for `BOKEH_ALLOW_WS_ORIGIN`. Copy this string, then uncomment the last two lines in the cell below, paste the alphanumeric string in place of the `INSERT_BOKEH_ALLOW_WS_ORIGIN_VALUE_HERE` text, run the cell, then try running the cell above to launch the explorer app again. It should work now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are following the steps above to run the explorer app in VS Code,\n",
    "# uncomment the following lines (remove the '#'s) before running this cell:\n",
    "import os\n",
    "os.environ[\"BOKEH_ALLOW_WS_ORIGIN\"] = \"0jubpfudr7ckf8qfh6dong6lr67pqrvbr5ugu8db8kcm4g6se70e\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of frame-by-frame pose drawing\n",
    "\n",
    "The cell below uses a different viz library to draw the poses in each successive frame on an HTML canvas, at the same frame rate as the source video.\n",
    "\n",
    "Note that this drawing library (`ipycanvas`) doesn't play well with the Bokeh interactive application above, which is why the somewhat clunkier PIL ImageDraw library is used to draw the poses there instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "canvas = Canvas(width=video_width, height=video_height, sync_image_data=True)\n",
    "\n",
    "display(canvas)\n",
    "\n",
    "\n",
    "def draw_frame_on_canvas(frame, canvas):\n",
    "\n",
    "    for pose_prediction in frame[\"predictions\"]:\n",
    "        pose_coords = np.array_split(\n",
    "            pose_prediction[\"keypoints\"], len(pose_prediction[\"keypoints\"]) / 3\n",
    "        )\n",
    "\n",
    "        for i, seg in enumerate(OPP_COCO_SKELETON):\n",
    "\n",
    "            if pose_coords[seg[0] - 1][2] == 0 or pose_coords[seg[1] - 1][2] == 0:\n",
    "                continue\n",
    "\n",
    "            canvas.stroke_style = OPP_COCO_COLORS[i]\n",
    "            canvas.line_width = 2\n",
    "\n",
    "            canvas.stroke_line(\n",
    "                pose_coords[seg[0] - 1][0],\n",
    "                pose_coords[seg[0] - 1][1],\n",
    "                pose_coords[seg[1] - 1][0],\n",
    "                pose_coords[seg[1] - 1][1],\n",
    "            )\n",
    "\n",
    "\n",
    "# This will \"animate\" all of the detected poses starting from the beginning of the video\n",
    "for frame in pose_data:\n",
    "\n",
    "    with hold_canvas():\n",
    "\n",
    "        canvas.clear()\n",
    "\n",
    "        draw_frame_on_canvas(frame, canvas)\n",
    "\n",
    "        sleep(1 / video_fps)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
