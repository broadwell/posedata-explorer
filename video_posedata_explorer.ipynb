{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A notebook for exploring video posedata\n",
    "\n",
    "**Intended use:** the user selects a video that is accompanied by already extracted posedata in a .json file. The notebook provides visualizations that summarize the quality and content of the poses extracted across all frames of the video, as well as armature plots of the detected poses in a selected frame. These can be viewed separately from the source video and even animated.\n",
    "\n",
    "Note that at present, this only works with .json output files generated via the Open PifPaf command-line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import (\n",
    "    Button,\n",
    "    CrosshairTool,\n",
    "    DatetimeTickFormatter,\n",
    "    Div,\n",
    "    LegendItem,\n",
    "    Line,\n",
    "    LinearAxis,\n",
    "    Range1d,\n",
    "    Slider,\n",
    "    Span,\n",
    "    TapTool,\n",
    "    Toggle,\n",
    ")\n",
    "from bokeh.models.sources import ColumnDataSource\n",
    "from bokeh.models.widgets.inputs import Select\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.themes import Theme\n",
    "import cv2\n",
    "import faiss\n",
    "from ipyfilechooser import FileChooser\n",
    "from IPython.display import HTML, display, clear_output\n",
    "from ipywidgets import Dropdown, Layout\n",
    "import jsonlines\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and display the video/posedata selector widget\n",
    "\n",
    "Clicking the \"Select\" button that appears after running this cell will display a filesystem navigator/selector widget that can be used to select a video for analysis. Note that for now, this video **must** be in the same folder as its posedata output, and the names of the matched video and posedata files should be identical, other than that the posedata file will have .openpifpaf.json appended to the name of the video file.\n",
    "\n",
    "The default folder the selector widget shows first is either the value of the `$DEV_FOLDER` environment variable (see README.md for information about how to set this via a `.env` file) or else the folder from which the notebook is being run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_folder = Path(os.getenv(\"DATA_FOLDER\", Path.cwd()))\n",
    "\n",
    "\n",
    "def get_available_videos(data_folder):\n",
    "    \"\"\"\n",
    "    Available videos will be limited to those with a .json and matching video (.mp4, .avi, etc)\n",
    "    file in a predefined directory (the notebook's running directory, for now)\n",
    "    \"\"\"\n",
    "    available_json_files = list(data_folder.glob(\"*.json\"))\n",
    "    available_video_files = (\n",
    "        p.resolve()\n",
    "        for p in Path(data_folder).glob(\"*\")\n",
    "        if p.suffix in {\".avi\", \".mp4\", \".mov\", \".mkv\", \".webm\"}\n",
    "    )\n",
    "    available_json = [\n",
    "        json_file.stem.split(\".\")[0] for json_file in available_json_files\n",
    "    ]\n",
    "\n",
    "    available_videos = []\n",
    "\n",
    "    for video_name in available_video_files:\n",
    "        if video_name.stem.split(\".\")[0] in available_json:\n",
    "            available_videos.append(video_name.name)\n",
    "\n",
    "    return available_videos\n",
    "\n",
    "\n",
    "fc = FileChooser(source_data_folder)\n",
    "fc.title = '<b>Use \"Select\" to choose a video file.</b><br>It must have an accompanying .openpifpaf.json file in the same folder.'\n",
    "fc.filter_pattern = ['*.mp4', '*.mkv', '*.avi', '*.webm', '*.mov']\n",
    "\n",
    "display(fc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect video and per-frame pose metadata for the selected video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_file = f\"{fc.selected}.openpifpaf.json\"\n",
    "video_file = fc.selected\n",
    "\n",
    "print(\"Video file:\", video_file)\n",
    "print(\"Posedata file:\", pose_file)\n",
    "\n",
    "cap = cv2.VideoCapture(video_file)\n",
    "video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(\"Video FPS:\", video_fps)\n",
    "\n",
    "print(\"Processing video and JSON files, please wait...\")\n",
    "\n",
    "pose_json = jsonlines.open(pose_file)\n",
    "pose_data = []\n",
    "\n",
    "# Per-frame pose data: frame, seconds, num_poses, avg_pose_conf, avg_coords_per_pose\n",
    "pose_series = {\n",
    "    \"frame\": [],\n",
    "    \"seconds\": [],\n",
    "    \"timestamp\": [],\n",
    "    \"num_poses\": [],\n",
    "    \"avg_score\": [],\n",
    "    \"avg_coords_per_pose\": [],\n",
    "}\n",
    "\n",
    "for frame in pose_json:\n",
    "\n",
    "    pose_data.append(frame)\n",
    "\n",
    "    # Frame output is numbered from 1 in the JSON\n",
    "    seconds = float(frame[\"frame\"] - 1) / video_fps\n",
    "\n",
    "    num_poses = len(frame[\"predictions\"])\n",
    "    pose_series[\"num_poses\"].append(num_poses)\n",
    "\n",
    "    pose_series[\"frame\"].append(frame[\"frame\"] - 1)\n",
    "    pose_series[\"seconds\"].append(seconds)\n",
    "\n",
    "    # Construct a timestamp that can be used with Bokeh's DatetimeTickFormatter\n",
    "    td = timedelta(seconds=seconds)\n",
    "    datestring = str(td)\n",
    "    if td.microseconds == 0:\n",
    "        datestring += \".000000\"\n",
    "    dt = datetime.strptime(datestring, \"%H:%M:%S.%f\")\n",
    "\n",
    "    pose_series[\"timestamp\"].append(dt)\n",
    "\n",
    "    pose_scores = []\n",
    "    pose_coords_counts = []\n",
    "    avg_score = 0  # NaN for empty frames?\n",
    "    avg_coords_per_pose = 0\n",
    "\n",
    "    normalized_poses = []\n",
    "\n",
    "    for pose in frame[\"predictions\"]:\n",
    "\n",
    "        pose_scores.append(pose[\"score\"])\n",
    "        pose_coords = 0\n",
    "        for i in range(0, len(pose[\"keypoints\"]), 3):\n",
    "            if pose[\"keypoints\"][i + 2] != 0:\n",
    "                pose_coords += 1\n",
    "\n",
    "        # To find the typically small proportion of poses that are complete\n",
    "        # if pose_coords == 17:\n",
    "        #     print(frame['frame'])\n",
    "\n",
    "        pose_coords_counts.append(pose_coords)\n",
    "\n",
    "    if num_poses > 0:\n",
    "        avg_score = sum(pose_scores) / num_poses\n",
    "        avg_coords_per_pose = sum(pose_coords_counts) / num_poses\n",
    "\n",
    "    pose_series[\"avg_score\"].append(avg_score)\n",
    "    pose_series[\"avg_coords_per_pose\"].append(avg_coords_per_pose)\n",
    "\n",
    "print(\"Total frames:\", len(pose_series[\"frame\"]))\n",
    "\n",
    "print(\"Duration:\", pose_series[\"timestamp\"][len(pose_series[\"timestamp\"]) - 1].time())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for pose plotting, represenation, comparison, manipulation\n",
    "\n",
    "These will be in a separate code library, eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The body part numberings and armature connectors for the 17-keypoint COCO pose format are defined in\n",
    "# https://github.com/openpifpaf/openpifpaf/blob/main/src/openpifpaf/plugins/coco/constants.py\n",
    "# Note that the body part numbers in the connector (skeleton) definitions begin with 1, for some reason, not 0\n",
    "OPP_COCO_SKELETON = [\n",
    "    (16, 14),\n",
    "    (14, 12),\n",
    "    (17, 15),\n",
    "    (15, 13),\n",
    "    (12, 13),\n",
    "    (6, 12),\n",
    "    (7, 13),\n",
    "    (6, 7),\n",
    "    (6, 8),\n",
    "    (7, 9),\n",
    "    (8, 10),\n",
    "    (9, 11),\n",
    "    (2, 3),\n",
    "    (1, 2),\n",
    "    (1, 3),\n",
    "    (2, 4),\n",
    "    (3, 5),\n",
    "    (4, 6),\n",
    "    (5, 7),\n",
    "]\n",
    "OPP_COCO_COLORS = [\n",
    "    \"orangered\",\n",
    "    \"orange\",\n",
    "    \"blue\",\n",
    "    \"lightblue\",\n",
    "    \"darkgreen\",\n",
    "    \"red\",\n",
    "    \"lightgreen\",\n",
    "    \"pink\",\n",
    "    \"plum\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"saddlebrown\",\n",
    "    \"mediumorchid\",\n",
    "    \"gray\",\n",
    "    \"salmon\",\n",
    "    \"chartreuse\",\n",
    "    \"lightgray\",\n",
    "    \"darkturquoise\",\n",
    "    \"goldenrod\",\n",
    "]\n",
    "\n",
    "UPSCALE = 5  # See draw_frame()\n",
    "\n",
    "# Default dimensions of the output visualizations (\"figure\" here simply means a graphic)\n",
    "FIGURE_WIDTH = 950\n",
    "FIGURE_HEIGHT = 500\n",
    "\n",
    "# Default dimension (length, width, maybe depth, eventually) of single pose viz\n",
    "POSE_MAX_DIM = 100\n",
    "\n",
    "# XXX ImageDraw does't ship with a scaleable font, so best to use matplotlib's\n",
    "font_path = os.path.join(\n",
    "    matplotlib.__path__[0], \"mpl-data\", \"fonts\", \"ttf\", \"DejaVuSans.ttf\"\n",
    ")\n",
    "try:\n",
    "    label_font = ImageFont.truetype(font_path, size=128)\n",
    "except:\n",
    "    label_font = None\n",
    "\n",
    "\n",
    "def unflatten_pose_data(prediction):\n",
    "    \"\"\"\n",
    "    Convert an Open PifPaf pose prediction (a 1D 51-element list) into a 17-element\n",
    "    list (not a NumPy array) of [x_coord, y_coord, confidence] triples.\n",
    "    \"\"\"\n",
    "    return np.array_split(prediction[\"keypoints\"], len(prediction[\"keypoints\"]) / 3)\n",
    "\n",
    "\n",
    "def extract_trustworthy_coords(prediction):\n",
    "    \"\"\"\n",
    "    Convert an Open PifPaf pose prediction from a 1D vector of coordinates and confidence\n",
    "    values to a 17x2 NumPy array containing only the armature coordinates, with coordinate values\n",
    "    set to NaN,NaN for any coordinate with a confidence value of 0.\n",
    "    Returns the 17x2 array and a separate list of the original confidence values.\n",
    "    \"\"\"\n",
    "    unflattened_pose = unflatten_pose_data(prediction)\n",
    "    trustworthy_coords = np.array([[coords[0], coords[1]] if coords[2] != 0 else [np.NaN, np.NaN] for coords in unflattened_pose]).flatten()\n",
    "    #confidences = [coords[3] for coords in unflattened_pose]\n",
    "    return trustworthy_coords\n",
    "\n",
    "\n",
    "def get_pose_extent(prediction):\n",
    "    \"\"\"Get the min and max x and y coordinates of an Open PifPaf pose prediction\"\"\"\n",
    "    pose_coords = unflatten_pose_data(prediction)\n",
    "    min_x = np.NaN\n",
    "    min_y = np.NaN\n",
    "    max_x = np.NaN\n",
    "    max_y = np.NaN\n",
    "    for coords in pose_coords:\n",
    "        # Coordinates with confidence values of 0 are not considered\n",
    "        if coords[2] == 0:\n",
    "            continue\n",
    "        min_x = np.nanmin([min_x, coords[0]])\n",
    "        min_y = np.nanmin([min_y, coords[1]])\n",
    "        max_x = np.nanmax([max_x, coords[0]])\n",
    "        max_y = np.nanmax([max_y, coords[1]])\n",
    "\n",
    "    return [min_x, min_y, max_x, max_y]\n",
    "\n",
    "\n",
    "def shift_pose_to_origin(prediction):\n",
    "    \"\"\"\n",
    "    Shift the keypoint coordinates of an Open PifPaf pose prediction so that the\n",
    "    min x and y coordinates of its extent are at the 0,0 origin.\n",
    "    NOTE: This only returns the modified 'keypoints' portion of the prediction.\n",
    "    \"\"\"\n",
    "    pose_coords = unflatten_pose_data(prediction)\n",
    "    min_x, min_y, max_x, max_y = get_pose_extent(prediction)\n",
    "\n",
    "    for i, coords in enumerate(pose_coords):\n",
    "        # Coordinates with confidence values of 0 are not modified; these should not\n",
    "        # be used in any pose representations or calculations, and often (but not\n",
    "        # always) already have 0,0 coordinates.\n",
    "        if coords[2] == 0:\n",
    "            continue\n",
    "        pose_coords[i] = [coords[0] - min_x, coords[1] - min_y, coords[2]]\n",
    "\n",
    "    return {\"keypoints\": np.concatenate(pose_coords, axis=None)}\n",
    "\n",
    "\n",
    "def rescale_pose_coords(prediction):\n",
    "    \"\"\"\n",
    "    Rescale the coordinates of an OpenPifPaf pose prediction so that the extent\n",
    "    of the pose's long axis is equal to the global POSE_MAX_DIM setting. The\n",
    "    coordinates of the short axis are scaled by the same factor, and then are\n",
    "    shifted so that the short axis is centered within the POSE_MAX_DIM extent.\n",
    "    NOTE: This only returns the modified 'keypoints' portion of the prediction.\n",
    "    \"\"\"\n",
    "    pose_coords = unflatten_pose_data(prediction)\n",
    "    min_x, min_y, max_x, max_y = get_pose_extent(prediction)\n",
    "\n",
    "    x_extent = max_x - min_x\n",
    "    y_extent = max_y - min_y\n",
    "\n",
    "    scale_factor = POSE_MAX_DIM / np.max([x_extent, y_extent])\n",
    "\n",
    "    if x_extent >= y_extent:\n",
    "        x_recenter = 0\n",
    "        y_recenter = round((POSE_MAX_DIM - (scale_factor * y_extent)) / 2)\n",
    "    else:\n",
    "        x_recenter = round((POSE_MAX_DIM - (scale_factor * x_extent)) / 2)\n",
    "        y_recenter = 0\n",
    "\n",
    "    for i, coords in enumerate(pose_coords):\n",
    "        # Coordinates with confidence values of 0 are not modified; these should not\n",
    "        # be used in any pose representations or calculations, and often (but not\n",
    "        # always) already have 0,0 coordinates.\n",
    "        if coords[2] == 0:\n",
    "            continue\n",
    "        pose_coords[i] = [\n",
    "            round(coords[0] * scale_factor + x_recenter),\n",
    "            round(coords[1] * scale_factor + y_recenter),\n",
    "            coords[2],\n",
    "        ]\n",
    "\n",
    "    return {\"keypoints\": np.concatenate(pose_coords, axis=None)}\n",
    "\n",
    "\n",
    "def shift_normalize_rescale_pose_coords(prediction):\n",
    "    \"\"\"\n",
    "    Convenience function to shift an Open PifPaf pose prediction so that its minimal corner\n",
    "    is at the origin, then rescale so that it fits into a POSE_MAX_DIM * POSE_MAX_DIM extent.\n",
    "    NOTE: This only returns the modified 'keypoints' portion of the prediction.\n",
    "    \"\"\"\n",
    "    return rescale_pose_coords(shift_pose_to_origin(prediction))\n",
    "\n",
    "\n",
    "def compare_poses_cosine(p1, p2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity of the 'keypoint' portions of two Open PifPaf pose predictions\n",
    "    by computing their cosine distance and subtracting this from 1 (so 1=identical).\n",
    "    \"\"\"\n",
    "    unflattened_p1 = unflatten_pose_data(p1)\n",
    "    return 1 - cosine(\n",
    "        np.array(unflatten_pose_data(p1))[:, :2].flatten(),\n",
    "        np.array(unflatten_pose_data(p2))[:, :2].flatten(),\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_joint_angles(prediction):\n",
    "    \"\"\"\n",
    "    Build an additional/alternative feature set for an Open PifPaf pose prediction, composed\n",
    "    of the angles, measured in radians, of several joints/articulation points on the body (see\n",
    "    list in code comments below).\n",
    "    \"\"\"\n",
    "    pose_coords = unflatten_pose_data(prediction)\n",
    "\n",
    "    joint_angles = []\n",
    "\n",
    "    # Joints to use:\n",
    "    joint_angle_points = [\n",
    "        [3, 5, 11],  # Left ear - left shoulder - left hip\n",
    "        [4, 6, 12],  # Right ear - right shoulder - right hip\n",
    "        [11, 5, 7],  # Left hip - left shoulder - left elbow\n",
    "        [12, 6, 8],  # Right hip - right shoulder - right elbow\n",
    "        [5, 7, 9],  # Left shoulder - left elbow - left wrist\n",
    "        [6, 8, 10],  # Right shoulder - right elbow - right wrist\n",
    "        [5, 11, 13],  # Left shoulder - left hip - left knee\n",
    "        [6, 12, 14],  # Right shoulder - right hip - right knee\n",
    "        [11, 13, 15],  # Left hip - left knee - left ankle\n",
    "        [12, 14, 16],  # Right hip - right knee - right ankle\n",
    "    ]\n",
    "\n",
    "    for angle_points in joint_angle_points:\n",
    "        # Need 3 points to make an angle; if 1 or more are missing, it's a NaN\n",
    "        if (\n",
    "            pose_coords[angle_points[0]][2] == 0\n",
    "            or pose_coords[angle_points[1]][2] == 0\n",
    "            or pose_coords[angle_points[2]][2] == 0\n",
    "        ):\n",
    "            joint_angles.append(np.NaN)\n",
    "        else:\n",
    "            ba = np.array(\n",
    "                [pose_coords[angle_points[0]][0], pose_coords[angle_points[0]][1]]\n",
    "            ) - np.array(\n",
    "                [pose_coords[angle_points[1]][0], pose_coords[angle_points[1]][1]]\n",
    "            )\n",
    "            bc = np.array(\n",
    "                [pose_coords[angle_points[2]][0], pose_coords[angle_points[2]][1]]\n",
    "            ) - np.array(\n",
    "                [pose_coords[angle_points[1]][0], pose_coords[angle_points[1]][1]]\n",
    "            )\n",
    "            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "            joint_angles.append(np.arccos(cosine_angle))\n",
    "\n",
    "    return joint_angles\n",
    "\n",
    "\n",
    "def compare_poses_angles(joint_angles1, joint_angles2):\n",
    "    \"\"\"\n",
    "    This computes a similarity score for two pose predictions that are represented\n",
    "    as vectors of joint angles. The similarity metric is essentially standard cosine\n",
    "    similarity (that tge values in the vectors are angle measurements does not make\n",
    "    a difference to how it works; they're just treated as numbers), modified to handle\n",
    "    missing/NaN vector values gracefully. (1=identical)\n",
    "    \"\"\"\n",
    "\n",
    "    angles_dot = np.nansum(np.array(joint_angles1) * np.array(joint_angles2))\n",
    "    angles_norm = np.sqrt(np.nansum(np.square(np.array(joint_angles1)))) * np.sqrt(\n",
    "        np.nansum(np.square(np.array(joint_angles2)))\n",
    "    )\n",
    "    return angles_dot / angles_norm\n",
    "\n",
    "\n",
    "def image_from_video_frame(video_file, frameno):\n",
    "    \"\"\"Grabs the specified frame from the video and converts it into an RGBA array\"\"\"\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    cap.set(1, frameno)\n",
    "    ret, img = cap.read()\n",
    "    rgb_bg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.cvtColor(rgb_bg, cv2.COLOR_RGB2RGBA)\n",
    "    image = np.asarray(img)\n",
    "    return image\n",
    "\n",
    "\n",
    "def extract_pose_background(pose_pred, video_file, pose_frameno):\n",
    "    min_x, min_y, max_x, max_y = get_pose_extent(pose_pred)\n",
    "\n",
    "    x_extent = max_x - min_x\n",
    "    y_extent = max_y - min_y\n",
    "\n",
    "    if x_extent >= y_extent:\n",
    "        x_padding = 0\n",
    "        y_padding = (x_extent - y_extent) / 2\n",
    "    else:\n",
    "        x_padding = (y_extent - x_extent) / 2\n",
    "        y_padding = 0\n",
    "\n",
    "    pose_frame_image = image_from_video_frame(video_file, pose_frameno)\n",
    "\n",
    "    # Add transparent letterboxing/pillarboxing pixels if a square cutout around\n",
    "    # the pose (needed for normalization) exceeds the frame of the image\n",
    "    x_start = round(min_x-x_padding)\n",
    "    x_stop = round(max_x+x_padding)                \n",
    "    y_start = round(min_y-y_padding)\n",
    "    y_stop = round(max_y+y_padding)\n",
    "\n",
    "    x_start_pad = 0\n",
    "    x_stop_pad = 0\n",
    "    y_start_pad = 0\n",
    "    y_stop_pad = 0\n",
    "\n",
    "    if x_start < 0:\n",
    "        x_start_pad = -x_start\n",
    "        x_start = 0\n",
    "    if x_stop >= pose_frame_image.shape[1]:\n",
    "        x_stop_pad = x_stop - pose_frame_image.shape[1]\n",
    "        x_stop = pose_frame_image.shape[1]-1\n",
    "    if y_start < 0:\n",
    "        y=y_start_pad = -y_start               \n",
    "        y_start = 0\n",
    "    if y_stop >= pose_frame_image.shape[0]:\n",
    "        y_stop_pad = y_stop - pose_frame_image.shape[0]\n",
    "        y_stop = pose_frame_image.shape[0]-1\n",
    "\n",
    "    pose_base_image = pose_frame_image[y_start:y_stop, x_start:x_stop]\n",
    "    if x_start_pad > 0:\n",
    "        pad_array = np.zeros((pose_base_image.shape[0], x_start_pad, 4), np.uint8)\n",
    "        pose_base_image = np.concatenate((pad_array, pose_base_image), axis=1)\n",
    "    if x_stop_pad > 0:\n",
    "        pad_array = np.zeros((pose_base_image.shape[0], x_stop_pad, 4), np.uint8)\n",
    "        pose_base_image = np.concatenate((pose_base_image, pad_array), axis=1)\n",
    "    if y_start_pad > 0:\n",
    "        pad_array = np.zeros((y_start_pad, pose_base_image.shape[1], 4), np.uint8)\n",
    "        pose_base_image = np.concatenate((pad_array, pose_base_image), axis=0)\n",
    "    if y_stop_pad > 0:\n",
    "        pad_array = np.zeros((y_stop_pad, pose_base_image.shape[1], 4), np.uint8)\n",
    "        pose_base_image = np.concatenate((pose_base_image, pad_array), axis=0)\n",
    "\n",
    "    return pose_base_image\n",
    "\n",
    "\n",
    "def add_pose_to_drawing(pose_prediction, drawing, seqno=None, show_bbox=False):\n",
    "\n",
    "    pose_coords = unflatten_pose_data(pose_prediction)\n",
    "\n",
    "    for i, seg in enumerate(OPP_COCO_SKELETON):\n",
    "\n",
    "        if pose_coords[seg[0] - 1][2] == 0 or pose_coords[seg[1] - 1][2] == 0:\n",
    "            continue\n",
    "\n",
    "        line_color = ImageColor.getrgb(OPP_COCO_COLORS[i])\n",
    "\n",
    "        segment_confidence = (pose_coords[seg[0] - 1][2] + pose_coords[seg[1] - 1][2]) / 2\n",
    "        line_color = line_color + (round(segment_confidence * 256),)\n",
    "\n",
    "        shape = [\n",
    "            (\n",
    "                int(pose_coords[seg[0] - 1][0] * UPSCALE),\n",
    "                int(pose_coords[seg[0] - 1][1] * UPSCALE),\n",
    "            ),\n",
    "            (\n",
    "                int(pose_coords[seg[1] - 1][0] * UPSCALE),\n",
    "                int(pose_coords[seg[1] - 1][1]) * UPSCALE,\n",
    "            ),\n",
    "        ]\n",
    "        drawing.line(shape, fill=line_color, width=2 * UPSCALE)\n",
    "\n",
    "    if \"bbox\" in pose_prediction:\n",
    "        bbox = pose_prediction[\"bbox\"]\n",
    "    else:\n",
    "        extent = get_pose_extent(pose_prediction)\n",
    "        bbox = [extent[0], extent[1], extent[2] - extent[0], extent[3] - extent[1]]\n",
    "\n",
    "    # bbox format for PifPaf is x0, y0, width, height\n",
    "    # Also note that both PifPaf and PIL/ImageDraw place (0,0) at top left, not bottom left\n",
    "    upper_left = (int(bbox[0] * UPSCALE), int(bbox[1] * UPSCALE))\n",
    "    lower_right = (\n",
    "        int((bbox[0] + bbox[2]) * UPSCALE),\n",
    "        int((bbox[1] + bbox[3]) * UPSCALE),\n",
    "    )\n",
    "\n",
    "    if show_bbox:\n",
    "        shape = [upper_left, lower_right]\n",
    "        drawing.rectangle(shape, outline=\"blue\", width=1 * UPSCALE)\n",
    "\n",
    "    if seqno is not None:\n",
    "        drawing.text(\n",
    "            upper_left, str(seqno + 1), font=label_font, align=\"right\", fill=\"blue\"\n",
    "        )\n",
    "\n",
    "    return drawing\n",
    "\n",
    "\n",
    "def add_unflattened_pose_to_drawing(pose_coords, drawing, line_prevalences=[]):\n",
    "    \"\"\"XXX need to DRY this up -- too much overlap with add_pose_to_drawing\"\"\"\n",
    "\n",
    "    for i, seg in enumerate(OPP_COCO_SKELETON):\n",
    "\n",
    "        if np.isnan(pose_coords[seg[0] - 1][0]) or np.isnan(pose_coords[seg[1] - 1][1]):\n",
    "            continue\n",
    "\n",
    "        line_color = ImageColor.getrgb(OPP_COCO_COLORS[i])\n",
    "\n",
    "        if len(line_prevalences):\n",
    "            line_color = line_color + (round(line_prevalences[i] * 256),)\n",
    "\n",
    "        shape = [\n",
    "            (\n",
    "                int(pose_coords[seg[0] - 1][0] * UPSCALE),\n",
    "                int(pose_coords[seg[0] - 1][1] * UPSCALE),\n",
    "            ),\n",
    "            (\n",
    "                int(pose_coords[seg[1] - 1][0] * UPSCALE),\n",
    "                int(pose_coords[seg[1] - 1][1]) * UPSCALE,\n",
    "            ),\n",
    "        ]\n",
    "        drawing.line(shape, fill=line_color, width=2 * UPSCALE)\n",
    "\n",
    "    return drawing\n",
    "\n",
    "\n",
    "def normalize_and_draw_pose(pose_prediction, already_normalized_and_unflattened=False, armature_prevalences=[], frameno=None):\n",
    "    if not already_normalized_and_unflattened:\n",
    "        original_prediction = pose_prediction\n",
    "        pose_prediction = shift_normalize_rescale_pose_coords(pose_prediction)\n",
    "    # Can also grab the background image and excerpt/scale it to match, if desired...\n",
    "    if frameno is not None and not already_normalized_and_unflattened:\n",
    "        # Get the frame image\n",
    "        bg_img = image_from_video_frame(video_file, frameno)\n",
    "        pose_base_image = extract_pose_background(original_prediction, video_file, frameno)\n",
    "        resized_image = cv2.resize(pose_base_image, dsize=(POSE_MAX_DIM*UPSCALE, POSE_MAX_DIM*UPSCALE), interpolation=cv2.INTER_LANCZOS4)\n",
    "        bg_img = Image.fromarray(resized_image)\n",
    "    else:\n",
    "        bg_img = Image.new(\"RGBA\", (POSE_MAX_DIM * UPSCALE, POSE_MAX_DIM * UPSCALE))\n",
    "    drawing = ImageDraw.Draw(bg_img)\n",
    "    if not already_normalized_and_unflattened:\n",
    "        drawing = add_pose_to_drawing(pose_prediction, drawing)\n",
    "    else:\n",
    "        drawing = add_unflattened_pose_to_drawing(pose_prediction, drawing, armature_prevalences)\n",
    "    if not already_normalized_and_unflattened:\n",
    "        bg_img = bg_img.resize(\n",
    "            (POSE_MAX_DIM, POSE_MAX_DIM), resample=Image.Resampling.LANCZOS\n",
    "        )\n",
    "    return bg_img\n",
    "\n",
    "\n",
    "def draw_frame(frame, bg_img=None):\n",
    "    \"\"\"Draws the poses in the specified frame, superimposing them on the frame image, if provided.\"\"\"\n",
    "\n",
    "    pixels_to_poses = {}\n",
    "\n",
    "    # The only way to get smooth(er) lines in the pose armatures via PIL ImageDraw is to upscale the entire\n",
    "    # image by some factor, draw the lines, then downscale back to the original resolution while applying\n",
    "    # Lanczos resampling, because ImageDraw doesn't do any native anti-aliasing.\n",
    "    if bg_img is None:\n",
    "        bg_img = Image.new(\"RGBA\", (video_width * UPSCALE, video_height * UPSCALE))\n",
    "    else:\n",
    "        bg_img = bg_img.resize((video_width * UPSCALE, video_height * UPSCALE))\n",
    "\n",
    "    drawing = ImageDraw.Draw(bg_img)\n",
    "\n",
    "    for i, pose_prediction in enumerate(frame[\"predictions\"]):\n",
    "\n",
    "        drawing = add_pose_to_drawing(pose_prediction, drawing, i, show_bbox=True)\n",
    "\n",
    "    bg_img = bg_img.resize(\n",
    "        (video_width, video_height), resample=Image.Resampling.LANCZOS\n",
    "    )\n",
    "\n",
    "    return bg_img\n",
    "\n",
    "\n",
    "def get_armature_prevalences(cluster_poses):\n",
    "    armature_appearances = [0] * len(OPP_COCO_SKELETON)\n",
    "    for pose_coords in cluster_poses:\n",
    "        pose_coords = np.array_split(pose_coords, len(pose_coords) / 2)\n",
    "\n",
    "        for i, seg in enumerate(OPP_COCO_SKELETON):\n",
    "            if not np.isnan(pose_coords[seg[0] - 1][0]) and not np.isnan(pose_coords[seg[1] - 1][1]):\n",
    "                armature_appearances[i] += 1\n",
    "    return [segcount / len(cluster_poses) for segcount in armature_appearances]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose normalization and clustering\n",
    "\n",
    "The following two cells need to be run to enable the pose search features of the posedata explorer app.\n",
    "\n",
    "The normalization process in the first cell can take quite a while if it has never been run on a particular set of video/posedata files (~10 minutes for a full-length play). But it then caches the results in pickle (*.p) files in the same folder as the video and posedata files, meaning it will take a very short amount of time on every subsequent invocation for that video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def progress_bar(value, max=100):\n",
    "    return HTML(\n",
    "        \"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(\n",
    "            value=value, max=max\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "normalized_pose_file = pose_file.replace(\".openpifpaf.json\", \".normalized.p\")\n",
    "metadata_file = pose_file.replace(\".openpifpaf.json\", \".metadata.p\")\n",
    "\n",
    "if (os.path.isfile(normalized_pose_file)) and (os.path.isfile(metadata_file)):\n",
    "    normalized_pose_data = pickle.load(open(normalized_pose_file, \"rb\"))\n",
    "    [normalized_pose_metadata, framepose_to_seqno] = pickle.load(open(metadata_file, \"rb\"))\n",
    "\n",
    "else:\n",
    "    print(\"Computing normalized poses for comparison and clustering\")\n",
    "    print(\"This may take a while...\")\n",
    "    bar = display(progress_bar(0, len(pose_data)), display_id=True)\n",
    "\n",
    "    # For cluster analysis, each pose must be a 1D array, and all poses must be in a 1D list\n",
    "    # that includes only the pose keypoint coordinates (not the confidence scores).\n",
    "    # So we also create a parallel data structure to keep track of the frame number and numbering\n",
    "    # within the frame of each of the poses.\n",
    "    normalized_pose_data = []\n",
    "    normalized_pose_metadata = []\n",
    "\n",
    "    framepose_to_seqno = {}\n",
    "    pose_seqno = 0\n",
    "\n",
    "    for i, frame in enumerate(pose_data):\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            bar.update(progress_bar(i, len(pose_data)))\n",
    "\n",
    "        for j, pose in enumerate(frame[\"predictions\"]):\n",
    "            normalized_coords = extract_trustworthy_coords(shift_normalize_rescale_pose_coords(pose))\n",
    "            normalized_pose_data.append(normalized_coords)\n",
    "            normalized_pose_metadata.append({\"frameno\": i, \"poseno\": j})\n",
    "\n",
    "            if i in framepose_to_seqno:\n",
    "                framepose_to_seqno[i][j] = pose_seqno\n",
    "            else:\n",
    "                framepose_to_seqno[i] = {j: pose_seqno}\n",
    "\n",
    "            pose_seqno += 1\n",
    "\n",
    "    pickle.dump(normalized_pose_data, open(normalized_pose_file, \"wb\"))\n",
    "    pickle.dump([normalized_pose_metadata, framepose_to_seqno], open(metadata_file, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS indexes of the poses, for fast nearest neighbor similarity search\n",
    "\n",
    "print(\"Indexing video posedata set for similarity search\")\n",
    "\n",
    "# FAISS can't handle NaNs in the iput vectors so use -1s instead\n",
    "faiss_pose_data = [tuple(np.nan_to_num(raw_pose, nan=-1).tolist()) for raw_pose in normalized_pose_data]\n",
    "\n",
    "# This builds an exact (flat) index based on Euclidean distance\n",
    "faiss_L2_index = faiss.IndexFlatL2(34)\n",
    "faiss_L2_input = np.array(faiss_pose_data).astype('float32')\n",
    "faiss_L2_index.add(faiss_L2_input)\n",
    "\n",
    "# The below builds an exact (flat) index based on inner-product distance,\n",
    "# which is equivalent to cosine similarity when the inputs are normalized.\n",
    "# So far, its results have not been noticeably preferable to the L2\n",
    "# (Euclidean) distance-based index, but it may be useful in the future.\n",
    "# faiss_IP_index = faiss.IndexFlatIP(34)\n",
    "# faiss_IP_input = np.array(faiss_pose_data).astype('float32')\n",
    "# faiss.normalize_L2(faiss_IP_input) # Must normalize the inputs!\n",
    "# faiss_IP_index.add(faiss_IP_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: cluster analysis of normalized poses\n",
    "\n",
    "The cell below computes a K-means clustering of the poses based on the L2 (Euclidean) similarities of their normalized coordinate vectors, then computes and visualizes the relative sizes of the clusters and the averages of their poses. The averaged poses are visualized on averages of their background (source) image regions if `AVERAGE_BACKGROUNDS` is set to `True` -- although it takes quite a bit longer to average the background images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL cluster analysis of the normalized poses:\n",
    "# Compute a K-means clustering of the poses based on the L2 (Euclidean) similarities\n",
    "# of their normalized coordinate vectors, then compute and visualize the sizes of\n",
    "# the clusters and the averages of their poses.\n",
    "NUMBER_OF_CLUSTERS = 100\n",
    "AVERAGE_BACKGROUNDS = True\n",
    "IMAGE_SAMPLE = 100 # Only contribute 1 in 100 images to the background average\n",
    "AVERAGE_BACKGROUNDS = True # Set to True to average the pose backgrounds\n",
    "\n",
    "print(f\"Clustering video poses into {NUMBER_OF_CLUSTERS} clusters\")\n",
    "kmeans_faiss = faiss.Kmeans(d=faiss_L2_input.shape[1], k=NUMBER_OF_CLUSTERS, niter=100)\n",
    "kmeans_faiss.train(faiss_L2_input)\n",
    "_, cluster_labels = kmeans_faiss.index.search(faiss_L2_input, 1)\n",
    "cluster_labels = np.array(cluster_labels).flatten()\n",
    "\n",
    "bin_counts = {}\n",
    "cluster_to_pose = {}\n",
    "\n",
    "for i in range(len(cluster_labels)):\n",
    "    ct = cluster_labels[i]\n",
    "    if ct not in bin_counts:\n",
    "        bin_counts[ct] = 1\n",
    "    else:\n",
    "        bin_counts[ct] += 1\n",
    "    if ct not in cluster_to_pose:\n",
    "        cluster_to_pose[ct] = [i]\n",
    "    else:\n",
    "        cluster_to_pose[ct].append(i)\n",
    "\n",
    "sorted_bin_counts = dict(sorted(bin_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_bin_counts_list = list(sorted_bin_counts.values())\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.bar(range(len(sorted_bin_counts_list)), sorted_bin_counts_list)\n",
    "plt.xlabel(\"cluster\")\n",
    "plt.ylabel(\"# poses\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for k in list(sorted_bin_counts.keys())[:10]:\n",
    "    cluster_poses = []\n",
    "    images_array = []\n",
    "\n",
    "    # Use some matplotlib weirdness to draw the stick figures in higher resolution\n",
    "    # but with the same axis labels (0-100 \"pixels\")\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(UPSCALE*100/fig.dpi, UPSCALE*100/fig.dpi)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    print(\"Cluster:\", k, \"| total poses:\", len(cluster_to_pose[k]))\n",
    "\n",
    "    for i, pose_id in enumerate(cluster_to_pose[k]):\n",
    "\n",
    "        cluster_poses.append(normalized_pose_data[pose_id])\n",
    "\n",
    "        # Don't average the background of every pose in the cluster,\n",
    "        # because that takes too long\n",
    "        if AVERAGE_BACKGROUNDS and (i % IMAGE_SAMPLE) == 0:\n",
    "            # Get the original posedata for the pose in order to extract the background image\n",
    "            pose_frameno = normalized_pose_metadata[pose_id]['frameno']\n",
    "            poseno = normalized_pose_metadata[pose_id]['poseno']\n",
    "            pose_pred = pose_data[pose_frameno][\"predictions\"][poseno]\n",
    "\n",
    "            pose_base_image = extract_pose_background(pose_pred, video_file, pose_frameno)\n",
    "\n",
    "            # Resize/normalize the cutout background dimensions, just as is done\n",
    "            # for the pose itself\n",
    "            resized_image = cv2.resize(pose_base_image, dsize=(POSE_MAX_DIM*UPSCALE, POSE_MAX_DIM*UPSCALE), interpolation=cv2.INTER_LANCZOS4)\n",
    "            images_array.append(resized_image)\n",
    "\n",
    "    if AVERAGE_BACKGROUNDS:\n",
    "        images_array = np.array(images_array, dtype=float)\n",
    "\n",
    "        # Average the RGB values of all of the pose background images\n",
    "        avg_background_img = np.mean(images_array, axis=0).astype(np.uint8)\n",
    "        plt.imshow(avg_background_img)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(action='ignore', message='Mean of empty slice')\n",
    "        cluster_average = np.nanmean(np.array(cluster_poses), axis=0).tolist()\n",
    "\n",
    "    armature_prevalences = get_armature_prevalences(cluster_poses)\n",
    "    cluster_average = np.array_split(cluster_average, len(cluster_average) / 2)\n",
    "    cluster_average_img = normalize_and_draw_pose(cluster_average, already_normalized_and_unflattened=True, armature_prevalences=armature_prevalences)\n",
    "\n",
    "    plt.imshow(cluster_average_img)\n",
    "    axis_labels = [0] + list(range(0, 100, 20))\n",
    "    axis_label_locs = [lab * UPSCALE for lab in axis_labels]\n",
    "\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(axis_label_locs))\n",
    "    ax.set_xticklabels(axis_labels)\n",
    "    ax.yaxis.set_major_locator(mticker.FixedLocator(axis_label_locs))\n",
    "    ax.set_yticklabels(axis_labels)\n",
    "    plt.show()    \n",
    "\n",
    "    # If we want to inspect some of the poses in the cluster\n",
    "    # for i in range (10):\n",
    "    #     this_pose = np.array_split(cluster_poses[i], len(cluster_poses[i]) / 2)\n",
    "    #     pose_img =  normalize_and_draw_pose(this_pose, already_normalized_and_unflattened=True)\n",
    "\n",
    "    #     plt.imshow(pose_img)\n",
    "    #     plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and launch the explorer app\n",
    "\n",
    "This displays an interactive chart visualization of the attributes of the posedata in the .json output file across the runtime of the video.\n",
    "\n",
    "Clicking anywhere in the chart, moving the slider, or clicking the prev/next buttons will select a frame and draw the poses detected in that frame, with the option of displaying the actual image from the source video as the \"background.\"\n",
    "\n",
    "Please see the cell below the next if you are running this notebook in VS Code. Note also that the Jupyter server must be running on port 8888 (or 8889) for the explorer app to work in Jupyter/JupterLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_bokeh_image(pil_img, target_width, target_height):\n",
    "    \"\"\"The Bokeh interactive notebook tools will only display image data if it's formatted in a particular way\"\"\"\n",
    "    img_array = np.array(pil_img.transpose(Image.Transpose.FLIP_TOP_BOTTOM))\n",
    "\n",
    "    img = np.empty(img_array.shape[:2], dtype=np.uint32)\n",
    "    view = img.view(dtype=np.uint8).reshape(img_array.shape)\n",
    "\n",
    "    for i in range(target_height):\n",
    "        for j in range(target_width):\n",
    "            view[i, j, 0] = img_array[i, j, 0]\n",
    "            view[i, j, 1] = img_array[i, j, 1]\n",
    "            view[i, j, 2] = img_array[i, j, 2]\n",
    "            view[i, j, 3] = img_array[i, j, 3]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def bkapp(doc):\n",
    "    \"\"\"Define and run the Bokeh interactive notebook (Python + Javascript) application\"\"\"\n",
    "\n",
    "    # Some session data is best stored in a global dictionary\n",
    "    data = {}\n",
    "\n",
    "    max_y = max(pose_series[\"avg_coords_per_pose\"] + pose_series[\"num_poses\"])\n",
    "\n",
    "    # This is the main interactive timeline chart\n",
    "    tl = figure(\n",
    "        width=FIGURE_WIDTH,\n",
    "        height=FIGURE_HEIGHT,\n",
    "        title=video_file,\n",
    "        min_border=10,\n",
    "        y_range=(0, max_y + 1),\n",
    "        tools=\"save,box_zoom,pan,reset\",\n",
    "    )\n",
    "    # Format the X axis as hour-minute-second timecodes\n",
    "    tl.x_range = Range1d(min(pose_series[\"timestamp\"]), max(pose_series[\"timestamp\"]))\n",
    "    tl.xaxis.axis_label = \"Time\"\n",
    "    time_formatter = DatetimeTickFormatter(\n",
    "        hourmin=\"%H:%M:%S\",\n",
    "        minutes=\"%H:%M:%S\",\n",
    "        minsec=\"%H:%M:%S\",\n",
    "        seconds=\"%Ss\",\n",
    "        milliseconds=\"%3Nms\",\n",
    "    )\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"num_poses\"],\n",
    "        legend_label=\"Poses per frame\",\n",
    "        color=\"blue\",\n",
    "        alpha=0.6,\n",
    "        line_width=2,\n",
    "    )\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"avg_coords_per_pose\"],\n",
    "        legend_label=\"Coords per pose\",\n",
    "        color=\"red\",\n",
    "        alpha=0.6,\n",
    "        line_width=2,\n",
    "    )\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        [0] * len(pose_series[\"frame\"]),\n",
    "        color=\"orange\",\n",
    "        alpha=0,\n",
    "        line_width=2,\n",
    "        name=\"similar_poses\"\n",
    "    )\n",
    "\n",
    "    # The left Y axis corresponds to counts of poses and coordinates\n",
    "    tl.yaxis.axis_label = \"Poses or Coords\"\n",
    "    tl.extra_y_ranges = {\"avg_score\": Range1d(0, 1)}\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"avg_score\"],\n",
    "        y_range_name=\"avg_score\",\n",
    "        legend_label=\"Avg pose score\",\n",
    "        color=\"green\",\n",
    "        alpha=0.4,\n",
    "        line_width=2,\n",
    "    )\n",
    "    # The right Y axis corresponds to the average pose score (from 0 to 1)\n",
    "    tl.add_layout(\n",
    "        LinearAxis(y_range_name=\"avg_score\", axis_label=\"Avg Pose Score / Cosine Similarity\"), \"right\"\n",
    "    )\n",
    "    tl.xaxis.formatter = time_formatter\n",
    "    tl.xaxis.ticker.desired_num_ticks = 10\n",
    "    tl.legend.click_policy = \"hide\"\n",
    "    frame_line = Span(\n",
    "        location=pose_series[\"timestamp\"][0],\n",
    "        dimension=\"height\",\n",
    "        line_color=\"red\",\n",
    "        line_width=3,\n",
    "    )\n",
    "    tl.add_layout(frame_line)\n",
    "\n",
    "    def tl_tap(event):\n",
    "        \"\"\"When the chart is clicked, move the slider to the appropriate frame\"\"\"\n",
    "        # This is to try to avoid selecting a new frame when clicking on the legend\n",
    "        # to show or hide a glyph. The legend overlaps with about 25% of the plot height.\n",
    "        if event.y > .75 * max_y:\n",
    "            return\n",
    "        # event.x is a timestamp, so it needs to be converted to a frameno\n",
    "        start_dt = datetime(1900, 1, 1)\n",
    "        dt = datetime.utcfromtimestamp(event.x / 1000)\n",
    "        t_delta = dt - start_dt\n",
    "        clicked_frame = round(t_delta.total_seconds() * video_fps)\n",
    "        slider_callback(None, slider.value, clicked_frame)\n",
    "\n",
    "    tl_tap_tool = TapTool()\n",
    "    tl_crosshair_tool = CrosshairTool()\n",
    "\n",
    "    def get_frame_info(fn):\n",
    "        return f\"Frame info: {pose_series['num_poses'][fn]} detected poses, {pose_series['avg_coords_per_pose'][fn]:.3f} avg coords/pose, {pose_series['avg_score'][fn]:.3f} avg pose score\"\n",
    "\n",
    "    info_div = Div(text=get_frame_info(0))\n",
    "\n",
    "    tl.add_tools(tl_tap_tool, tl_crosshair_tool)\n",
    "    tl.on_event(\"tap\", tl_tap)\n",
    "\n",
    "    # This is the second figure, where the poses in the selected frame are drawn\n",
    "    fr = figure(\n",
    "        x_range=(0, video_width),\n",
    "        y_range=(0, video_height),\n",
    "        width=FIGURE_WIDTH,\n",
    "        height=int(FIGURE_WIDTH / video_width * video_height),\n",
    "        title=\"Poses in selected frame\",\n",
    "        tools=\"save\",\n",
    "    )\n",
    "    # Add an invisible glyph to suppress the \"figure has no renderers\" warning\n",
    "    fr.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    pose_info_div = Div(text=\"Click to poses to compare\")\n",
    "\n",
    "    pose_p1 = figure(\n",
    "        x_range=(0, POSE_MAX_DIM),\n",
    "        y_range=(0, POSE_MAX_DIM),\n",
    "        width=POSE_MAX_DIM * 2,\n",
    "        height=POSE_MAX_DIM * 2,\n",
    "        title=\"\",\n",
    "        tools=\"\",\n",
    "    )\n",
    "    # Add an invisible glyph to suppress the \"figure has no renderers\" warning\n",
    "    pose_p1.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    pose_p2 = figure(\n",
    "        x_range=(0, POSE_MAX_DIM),\n",
    "        y_range=(0, POSE_MAX_DIM),\n",
    "        width=POSE_MAX_DIM * 2,\n",
    "        height=POSE_MAX_DIM * 2,\n",
    "        title=\"\",\n",
    "        tools=\"\",\n",
    "    )\n",
    "    # Add an invisible glyph to suppress the \"figure has no renderers\" warning\n",
    "    pose_p2.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    def background_toggle_handler(event):\n",
    "        \"\"\"When the image underlay is toggled on or off, prompt the slider to redraw the frame\"\"\"\n",
    "        slider_callback(None, slider.value, slider.value)\n",
    "\n",
    "    background_switch = Toggle(label=\"show background\", active=False)\n",
    "    background_switch.on_click(background_toggle_handler)\n",
    "\n",
    "    def slider_callback(attr, old, new):\n",
    "        \"\"\"When the slider moves, draw the poses in the new frame and show the background if desired\"\"\"\n",
    "        slider.value = new\n",
    "        fr.renderers = []\n",
    "        if background_switch.active:\n",
    "            rgba_bg = image_from_video_frame(video_file, new)\n",
    "            pil_bg = Image.fromarray(rgba_bg)\n",
    "            frame_img = draw_frame(pose_data[new], pil_bg)\n",
    "        else:\n",
    "            frame_img = draw_frame(pose_data[new])\n",
    "        img = pil_to_bokeh_image(frame_img, video_width, video_height)\n",
    "        fr.image_rgba(image=[img], x=0, y=0, dw=img.shape[1], dh=img.shape[0])\n",
    "        if old != new:\n",
    "            info_div.text = get_frame_info(new)\n",
    "            frame_line.location = pose_series[\"timestamp\"][new]\n",
    "            pose_p1.title.text = \"\"\n",
    "            pose_p1.renderers = []\n",
    "            pose_p2.title.text = \"\"\n",
    "            pose_p2.renderers = []\n",
    "            pose_info_div.text = \"Click two poses to compare\"\n",
    "            for pose_box in similar_poses:\n",
    "                pose_box.renderers = []\n",
    "\n",
    "    slider = Slider(\n",
    "        start=0, end=len(pose_data) - 1, value=0, step=1, title=\"Selected frame\"\n",
    "    )\n",
    "    slider.on_change(\"value_throttled\", slider_callback)\n",
    "\n",
    "    def get_pose_extent_maps(frameno):\n",
    "        pose_extent_maps = []\n",
    "        for i, pose_prediction in enumerate(pose_data[frameno][\"predictions\"]):\n",
    "\n",
    "            if \"bbox\" in pose_prediction:\n",
    "                bbox = pose_prediction[\"bbox\"]\n",
    "            else:\n",
    "                extent = get_pose_extent(pose_prediction)\n",
    "                bbox = [\n",
    "                    extent[0],\n",
    "                    extent[1],\n",
    "                    extent[2] - extent[0],\n",
    "                    extent[3] - extent[1],\n",
    "                ]\n",
    "\n",
    "            extent_map = {\n",
    "                \"poseno\": i,\n",
    "                \"min_x\": bbox[0],\n",
    "                \"min_y\": video_height - bbox[3] - bbox[1],\n",
    "                \"max_x\": bbox[0] + bbox[2],\n",
    "                \"max_y\": video_height - bbox[1],\n",
    "            }\n",
    "\n",
    "            pose_extent_maps.append(extent_map)\n",
    "\n",
    "        return pose_extent_maps\n",
    "\n",
    "    def match_pose_pixel_maps(x, y, pose_extent_maps):\n",
    "        matched_poses = []\n",
    "        for extent_map in pose_extent_maps:\n",
    "            if (\n",
    "                x >= extent_map[\"min_x\"]\n",
    "                and x <= extent_map[\"max_x\"]\n",
    "                and y >= extent_map[\"min_y\"]\n",
    "                and y <= extent_map[\"max_y\"]\n",
    "            ):\n",
    "                matched_poses.append(extent_map[\"poseno\"])\n",
    "        return matched_poses\n",
    "\n",
    "    def fr_tap(event):\n",
    "        \"\"\"When the frame is clicked\"\"\"\n",
    "        # event.x is a timestamp, so it needs to be converted to a frameno\n",
    "        pixel_key = f\"{int(event.x)}, {int(event.y)}\"\n",
    "        pose_extent_maps = get_pose_extent_maps(slider.value)\n",
    "        clicked_poses = match_pose_pixel_maps(event.x, event.y, pose_extent_maps)\n",
    "        if len(clicked_poses):\n",
    "            pose_img = normalize_and_draw_pose(\n",
    "                pose_data[slider.value][\"predictions\"][clicked_poses[0]]\n",
    "            )\n",
    "            pose_img = pil_to_bokeh_image(pose_img, POSE_MAX_DIM, POSE_MAX_DIM)\n",
    "\n",
    "            if pose_p1.title.text == \"\":\n",
    "                pose_p1.image_rgba(\n",
    "                    image=[pose_img],\n",
    "                    x=0,\n",
    "                    y=0,\n",
    "                    dw=pose_img.shape[1],\n",
    "                    dh=pose_img.shape[0],\n",
    "                )\n",
    "                pose_p1.title = f\"{clicked_poses[0]+1}\"\n",
    "                pose_info_div.text = \"Please click another pose for comparison\"\n",
    "            elif pose_p1.title.text != \"\" and pose_p2.title.text == \"\":\n",
    "                pose_p2.image_rgba(\n",
    "                    image=[pose_img],\n",
    "                    x=0,\n",
    "                    y=0,\n",
    "                    dw=pose_img.shape[1],\n",
    "                    dh=pose_img.shape[0],\n",
    "                )\n",
    "                pose_p2.title = f\"{clicked_poses[0]+1}\"\n",
    "\n",
    "                normalized_p1 = shift_normalize_rescale_pose_coords(\n",
    "                    pose_data[slider.value][\"predictions\"][int(pose_p1.title.text) - 1]\n",
    "                )\n",
    "                normalized_p2 = shift_normalize_rescale_pose_coords(\n",
    "                    pose_data[slider.value][\"predictions\"][int(pose_p2.title.text) - 1]\n",
    "                )\n",
    "\n",
    "                cosine_similarity = compare_poses_cosine(\n",
    "                    normalized_p1,\n",
    "                    normalized_p2,\n",
    "                )\n",
    "                p1_angles = compute_joint_angles(normalized_p1)\n",
    "                p2_angles = compute_joint_angles(normalized_p2)\n",
    "                pose_p2.title = f\"{clicked_poses[0]+1}\"\n",
    "                angle_similarity = compare_poses_angles(p1_angles, p2_angles)\n",
    "                pose_info_div.text = f\"Cosine similarity between pose keypoints: {(cosine_similarity*100):3.3f}% | Similarity between pose joint angles: {(angle_similarity*100):3.3f}%\"\n",
    "\n",
    "    fr_tap_tool = TapTool()\n",
    "\n",
    "    fr.add_tools(fr_tap_tool)\n",
    "    fr.on_event(\"tap\", fr_tap)\n",
    "\n",
    "    def prev_handler(event):\n",
    "        slider_callback(None, slider.value, max(0, slider.value - 1))\n",
    "\n",
    "    def next_handler(event):\n",
    "        slider_callback(None, slider.value, min(slider.value + 1, len(pose_data) - 1))\n",
    "\n",
    "    prev_button = Button(label=\"prev\")\n",
    "    prev_button.on_click(prev_handler)\n",
    "    next_button = Button(label=\"next\")\n",
    "    next_button.on_click(next_handler)\n",
    "\n",
    "    search_info_div = Div(text=\"L2 (Euclidean distance) similar pose search\")\n",
    "\n",
    "    SIMILAR_POSES_TO_SHOW = 4\n",
    "    SIMILAR_MATCHES_TO_FIND = 1000\n",
    "    POSE_SIMILARITY_THRESHOLD = 0.8\n",
    "    similar_poses = []\n",
    "\n",
    "    for s in range(SIMILAR_POSES_TO_SHOW):\n",
    "        similar_poses.append(\n",
    "            figure(\n",
    "                x_range=(0, POSE_MAX_DIM),\n",
    "                y_range=(0, POSE_MAX_DIM),\n",
    "                width=POSE_MAX_DIM * 2,\n",
    "                height=POSE_MAX_DIM * 2,\n",
    "                title=\"\",\n",
    "                tools=\"\",\n",
    "            )\n",
    "        )\n",
    "    for pose_box in similar_poses:\n",
    "        pose_box.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    # Need to kep track of match data for paging through results\n",
    "    data['match_indices'] = None\n",
    "    data['valid_search_results'] = 0\n",
    "    data['search_results_index'] = 0\n",
    "    similar_frame_scores = [0] * len(pose_series[\"frame\"])\n",
    "    match_cosine_similarities = {}\n",
    "    target_frameno = None\n",
    "    target_poseno = None\n",
    "\n",
    "    def draw_similar_poses(start_rank):\n",
    "\n",
    "        # Clear any previously drawn poses\n",
    "        for pose_box in similar_poses:\n",
    "            pose_box.renderers = []\n",
    "\n",
    "        matches_to_show = 0\n",
    "        data['search_results_index'] = start_rank\n",
    "        \n",
    "        match_framenos = []\n",
    "        match_scores = []\n",
    "        match_timecodes = []\n",
    "        matches_advanced = 0\n",
    "\n",
    "        while matches_to_show < SIMILAR_POSES_TO_SHOW:\n",
    "            \n",
    "            current_match_rank = matches_advanced + start_rank\n",
    "            matches_advanced += 1\n",
    "\n",
    "            match_index = data['match_indices'][current_match_rank]\n",
    "            match_frameno = normalized_pose_metadata[match_index]['frameno']\n",
    "            match_poseno = normalized_pose_metadata[match_index]['poseno']\n",
    "\n",
    "            # Skip the self match, which is *usually* the first match, but may not\n",
    "            # always be due to approximations made in the indexing process\n",
    "            if target_frameno == match_frameno and target_poseno == match_poseno:\n",
    "                continue\n",
    "\n",
    "            matches_to_show += 1\n",
    "\n",
    "            match_framenos.append(str(match_frameno))\n",
    "            match_scores.append(f\"{match_cosine_similarities[match_index]*100:3.3f}%\")\n",
    "            match_timecodes.append(str(timedelta(seconds=pose_series['seconds'][match_frameno])).replace('0000',''))\n",
    "\n",
    "            if background_switch.active:\n",
    "                match_img = normalize_and_draw_pose(\n",
    "                    pose_data[match_frameno][\"predictions\"][match_poseno], False, [], match_frameno\n",
    "                )\n",
    "            else:\n",
    "                match_img = normalize_and_draw_pose(\n",
    "                    pose_data[match_frameno][\"predictions\"][match_poseno]\n",
    "                )\n",
    "\n",
    "            match_img = pil_to_bokeh_image(match_img, POSE_MAX_DIM, POSE_MAX_DIM)\n",
    "\n",
    "            similar_poses[matches_to_show-1].image_rgba(\n",
    "                image=[match_img],\n",
    "                x=0,\n",
    "                y=0,\n",
    "                dw=match_img.shape[1],\n",
    "                dh=match_img.shape[0],\n",
    "            )\n",
    "\n",
    "        search_info_div.text = f\"matches in frames {', '.join(match_framenos)} | {', '.join(match_timecodes)} | scores {', '.join(match_scores)}\"\n",
    "\n",
    "    def find_similar_poses():\n",
    "        \"\"\"\n",
    "        NOTE that although the FAISS search should always return the query pose as the\n",
    "        first search result, we don't want to include it in the displayed results,\n",
    "        hence the +1s and -1s in the code below.\n",
    "        \"\"\"\n",
    "        if pose_p1.title.text == \"\":\n",
    "            return\n",
    "\n",
    "        for pose_box in similar_poses:\n",
    "            pose_box.renderers = []\n",
    "\n",
    "        target_frameno = slider.value\n",
    "        target_poseno = int(pose_p1.title.text)-1\n",
    "\n",
    "        target_pose_w_confs = shift_normalize_rescale_pose_coords(\n",
    "            pose_data[target_frameno][\"predictions\"][target_poseno]\n",
    "        )\n",
    "        target_pose = extract_trustworthy_coords(target_pose_w_confs)\n",
    "\n",
    "        target_pose_query = np.array([np.nan_to_num(target_pose, nan=-1)]).astype(\"float32\")\n",
    "\n",
    "        D, I = faiss_L2_index.search(target_pose_query, SIMILAR_MATCHES_TO_FIND)\n",
    "        \n",
    "        data['match_indices'] = I[0]\n",
    "        data['valid_search_results'] = 0\n",
    "        data['search_results_index'] = 0\n",
    "        similar_frame_scores = [0] * len(pose_series[\"frame\"])\n",
    "\n",
    "        for m in range(SIMILAR_MATCHES_TO_FIND):\n",
    "            match_index = data['match_indices'][m]\n",
    "            if match_index != -1:\n",
    "                match_frameno = normalized_pose_metadata[match_index]['frameno']\n",
    "                match_poseno = normalized_pose_metadata[match_index]['poseno']\n",
    "\n",
    "                cosine_similarity = compare_poses_cosine(\n",
    "                    target_pose_w_confs,\n",
    "                    shift_normalize_rescale_pose_coords(pose_data[match_frameno]['predictions'][match_poseno])\n",
    "                )\n",
    "\n",
    "                match_cosine_similarities[match_index] = cosine_similarity\n",
    "                \n",
    "                if cosine_similarity >= POSE_SIMILARITY_THRESHOLD:\n",
    "                    if similar_frame_scores[match_frameno] > 0:\n",
    "                        similar_frame_scores[match_frameno] = max(similar_frame_scores[match_frameno], cosine_similarity)\n",
    "                    else:\n",
    "                        similar_frame_scores[match_frameno] = cosine_similarity\n",
    "\n",
    "                    data['valid_search_results'] += 1\n",
    "\n",
    "        similar_poses_renderers = tl.select(name=\"similar_poses\")\n",
    "        if len(similar_poses_renderers)>0:\n",
    "            for sim_pose_renderer in similar_poses_renderers:\n",
    "                try:\n",
    "                    tl.renderers.remove(sim_pose_renderer)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "        tl.line(\n",
    "            pose_series[\"timestamp\"],\n",
    "            similar_frame_scores,\n",
    "            y_range_name=\"avg_score\",\n",
    "            legend_label=\"Similar poses\",\n",
    "            color=\"orange\",\n",
    "            alpha=0.8,\n",
    "            line_width=2,\n",
    "            name=\"similar_poses\"\n",
    "        )\n",
    "        \n",
    "    def find_and_draw_similar_poses():\n",
    "        find_similar_poses()\n",
    "        draw_similar_poses(0)\n",
    "\n",
    "    def get_similar_poses_handler(event):\n",
    "        search_info_div.text = \"<strong>Searching for similar poses, please wait...</strong>\"\n",
    "        doc.add_next_tick_callback(find_and_draw_similar_poses)\n",
    "\n",
    "    def reset_subposes_handler(event):\n",
    "        pose_p1.title.text = \"\"\n",
    "        pose_p1.renderers = []\n",
    "        pose_p2.title.text = \"\"\n",
    "        pose_p2.renderers = []\n",
    "        pose_info_div.text = \"\"\n",
    "        for pose_box in similar_poses:\n",
    "            pose_box.renderers = []\n",
    "        similar_poses_renderers = tl.select(name=\"similar_poses\")\n",
    "        if len(similar_poses_renderers)>0:\n",
    "            for sim_pose_renderer in similar_poses_renderers:\n",
    "                try:\n",
    "                    tl.renderers.remove(sim_pose_renderer)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        for li in tl.legend.items:\n",
    "            if li.label[\"value\"] == \"Similar poses\":\n",
    "                li.visible = False\n",
    "        search_info_div.text = \"L2 (Euclidean distance) similar pose search\"\n",
    "\n",
    "    reset_subposes_button = Button(label=\"clear\")\n",
    "    reset_subposes_button.on_click(reset_subposes_handler)\n",
    "\n",
    "    get_similar_poses_button = Button(label=\"look up 1st pose\")\n",
    "    get_similar_poses_button.on_click(get_similar_poses_handler)\n",
    "\n",
    "    frame_control_row = row(children=[prev_button, next_button, background_switch])\n",
    "\n",
    "    pose_buttons_column = column(reset_subposes_button, get_similar_poses_button)\n",
    "\n",
    "    subposes_row = row(children=[pose_p1, pose_p2, pose_buttons_column])\n",
    "\n",
    "    def prev_similar_poses_handler(event):\n",
    "        draw_similar_poses(max(0,data['search_results_index']-SIMILAR_POSES_TO_SHOW))\n",
    "    \n",
    "    def next_similar_poses_handler(event):\n",
    "        draw_similar_poses(min(data['search_results_index']+SIMILAR_POSES_TO_SHOW,data['valid_search_results']-SIMILAR_POSES_TO_SHOW))\n",
    "\n",
    "    prev_similar_button = Button(label=\"previous group of poses\")\n",
    "    prev_similar_button.on_click(prev_similar_poses_handler)\n",
    "\n",
    "    next_similar_button = Button(label=\"next group of poses\")\n",
    "    next_similar_button.on_click(next_similar_poses_handler)\n",
    "\n",
    "    similar_poses_controls = row(children=[prev_similar_button, next_similar_button])\n",
    "\n",
    "    similar_poses_row = row(children=similar_poses)\n",
    "\n",
    "    layout_column = column(\n",
    "        tl,\n",
    "        slider,\n",
    "        info_div,\n",
    "        frame_control_row,\n",
    "        fr,\n",
    "        pose_info_div,\n",
    "        subposes_row,\n",
    "        search_info_div,\n",
    "        similar_poses_controls,\n",
    "        similar_poses_row,\n",
    "    )\n",
    "\n",
    "    doc.add_root(layout_column)\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "show(bkapp, notebook_url=\"localhost:8889\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the notebook in VS Code:** As of early 2023, if you are running this notebook in VS Code instead of Jupyter or JupyterLab, the above cell will not work (BokehJS will load, but no figures will appear) without the following workaround:\n",
    "\n",
    "Take note of the error message that appears when you try to run the cell above, particularly the long alphanumeric string suggested as a value for `BOKEH_ALLOW_WS_ORIGIN`. Copy this string, then uncomment the last two lines in the cell below, paste the alphanumeric string in place of the `INSERT_BOKEH_ALLOW_WS_ORIGIN_VALUE_HERE` text, run the cell, then try running the cell above to launch the explorer app again. It should work now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are following the steps above to run the explorer app in VS Code,\n",
    "# uncomment the following lines (remove the '#'s) before running this cell:\n",
    "import os\n",
    "os.environ[\"BOKEH_ALLOW_WS_ORIGIN\"] = \"0jubpfudr7ckf8qfh6dong6lr67pqrvbr5ugu8db8kcm4g6se70e\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of frame-by-frame pose drawing\n",
    "\n",
    "The cell below uses a different viz library to draw the poses in each successive frame on an HTML canvas, at the same frame rate as the source video.\n",
    "\n",
    "Note that this drawing library (`ipycanvas`) doesn't play well with the Bokeh interactive application above, which is why the somewhat clunkier PIL ImageDraw library is used to draw the poses there instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "canvas = Canvas(width=video_width, height=video_height, sync_image_data=True)\n",
    "\n",
    "display(canvas)\n",
    "\n",
    "\n",
    "def draw_frame_on_canvas(frame, canvas):\n",
    "\n",
    "    for pose_prediction in frame[\"predictions\"]:\n",
    "        pose_coords = np.array_split(\n",
    "            pose_prediction[\"keypoints\"], len(pose_prediction[\"keypoints\"]) / 3\n",
    "        )\n",
    "\n",
    "        for i, seg in enumerate(OPP_COCO_SKELETON):\n",
    "\n",
    "            if pose_coords[seg[0] - 1][2] == 0 or pose_coords[seg[1] - 1][2] == 0:\n",
    "                continue\n",
    "\n",
    "            canvas.stroke_style = OPP_COCO_COLORS[i]\n",
    "            canvas.line_width = 2\n",
    "\n",
    "            canvas.stroke_line(\n",
    "                pose_coords[seg[0] - 1][0],\n",
    "                pose_coords[seg[0] - 1][1],\n",
    "                pose_coords[seg[1] - 1][0],\n",
    "                pose_coords[seg[1] - 1][1],\n",
    "            )\n",
    "\n",
    "\n",
    "# This will \"animate\" all of the detected poses starting from the beginning of the video\n",
    "for frame in pose_data:\n",
    "\n",
    "    with hold_canvas():\n",
    "\n",
    "        canvas.clear()\n",
    "\n",
    "        draw_frame_on_canvas(frame, canvas)\n",
    "\n",
    "        sleep(1 / video_fps)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
