{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A notebook for exploring video posedata\n",
    "\n",
    "**Intended use:** the user selects a video that is accompanied by already extracted posedata in a .json file. The notebook provides visualizations that summarize the quality and content of the poses extracted across all frames of the video, as well as armature plots of the detected poses in a selected frame. These can be viewed separately from the source video, compared numerically, grouped and searched by similarity, and even animated.\n",
    "\n",
    "Note that at present, this only works with .json output files generated via the Open PifPaf command-line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import warnings\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import (\n",
    "    Button,\n",
    "    CrosshairTool,\n",
    "    DatetimeTickFormatter,\n",
    "    Div,\n",
    "    LegendItem,\n",
    "    Line,\n",
    "    LinearAxis,\n",
    "    Range1d,\n",
    "    Slider,\n",
    "    Span,\n",
    "    TapTool,\n",
    "    Toggle,\n",
    ")\n",
    "from bokeh.models.sources import ColumnDataSource\n",
    "from bokeh.models.widgets.inputs import Select\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.themes import Theme\n",
    "import faiss\n",
    "from ipyfilechooser import FileChooser\n",
    "from IPython.display import HTML, display, clear_output\n",
    "from ipywidgets import Dropdown, Layout\n",
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "\n",
    "from pose_functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and display the video/posedata selector widget\n",
    "\n",
    "Clicking the \"Select\" button that appears after running this cell will display a filesystem navigator/selector widget that can be used to select a video for analysis. Note that for now, this video **must** be in the same folder as its posedata output, and the names of the matched video and posedata files should be identical, other than that the posedata file will have `.openpifpaf.json` appended to the name of the video file.\n",
    "\n",
    "The default folder the selector widget shows first is either the value of the `$DEV_FOLDER` environment variable (see README.md for information about how to set this via a `.env` file) or else the folder from which the notebook is being run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_folder = Path(os.getenv(\"DATA_FOLDER\", Path.cwd()))\n",
    "\n",
    "\n",
    "def get_available_videos(data_folder):\n",
    "    \"\"\"\n",
    "    Available videos will be limited to those with a .json and matching video (.mp4, .avi, etc)\n",
    "    file in a predefined directory (defaulting to the notebook's running directory)\n",
    "    \"\"\"\n",
    "    available_json_files = list(data_folder.glob(\"*.json\"))\n",
    "    available_video_files = (\n",
    "        p.resolve()\n",
    "        for p in Path(data_folder).glob(\"*\")\n",
    "        if p.suffix in {\".avi\", \".mp4\", \".mov\", \".mkv\", \".webm\"}\n",
    "    )\n",
    "    available_json = [\n",
    "        json_file.stem.split(\".\")[0] for json_file in available_json_files\n",
    "    ]\n",
    "\n",
    "    available_videos = []\n",
    "\n",
    "    for video_name in available_video_files:\n",
    "        if video_name.stem.split(\".\")[0] in available_json:\n",
    "            available_videos.append(video_name.name)\n",
    "\n",
    "    return available_videos\n",
    "\n",
    "\n",
    "fc = FileChooser(source_data_folder)\n",
    "fc.title = '<b>Use \"Select\" to choose a video file.</b><br>It must have an accompanying .openpifpaf.json file in the same folder.'\n",
    "fc.filter_pattern = [\"*.mp4\", \"*.mkv\", \"*.avi\", \"*.webm\", \"*.mov\"]\n",
    "\n",
    "display(fc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect video and per-frame pose metadata for the selected video\n",
    "\n",
    "Run this cell after selecting a video above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_file = f\"{fc.selected}.openpifpaf.json\"\n",
    "video_file = fc.selected\n",
    "\n",
    "print(\"Video file:\", video_file)\n",
    "print(\"Posedata file:\", pose_file)\n",
    "\n",
    "cap = cv2.VideoCapture(video_file)\n",
    "video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(\"Video FPS:\", video_fps)\n",
    "\n",
    "print(\"Processing video and JSON files, please wait...\")\n",
    "\n",
    "pose_json = jsonlines.open(pose_file)\n",
    "pose_data = []\n",
    "\n",
    "# Per-frame pose data: frame, seconds, num_poses, avg_pose_conf, avg_coords_per_pose\n",
    "pose_series = {\n",
    "    \"frame\": [],\n",
    "    \"seconds\": [],\n",
    "    \"timestamp\": [],\n",
    "    \"num_poses\": [],\n",
    "    \"avg_score\": [],\n",
    "    \"avg_coords_per_pose\": [],\n",
    "}\n",
    "\n",
    "for frame in pose_json:\n",
    "\n",
    "    pose_data.append(frame)\n",
    "\n",
    "    # Frame output is numbered from 1 in the JSON\n",
    "    seconds = float(frame[\"frame\"] - 1) / video_fps\n",
    "\n",
    "    num_poses = len(frame[\"predictions\"])\n",
    "    pose_series[\"num_poses\"].append(num_poses)\n",
    "\n",
    "    pose_series[\"frame\"].append(frame[\"frame\"] - 1)\n",
    "    pose_series[\"seconds\"].append(seconds)\n",
    "\n",
    "    # Construct a timestamp that can be used with Bokeh's DatetimeTickFormatter\n",
    "    td = timedelta(seconds=seconds)\n",
    "    datestring = str(td)\n",
    "    if td.microseconds == 0:\n",
    "        datestring += \".000000\"\n",
    "    dt = datetime.strptime(datestring, \"%H:%M:%S.%f\")\n",
    "\n",
    "    pose_series[\"timestamp\"].append(dt)\n",
    "\n",
    "    pose_scores = []\n",
    "    pose_coords_counts = []\n",
    "    avg_score = 0  # NaN for empty frames?\n",
    "    avg_coords_per_pose = 0\n",
    "\n",
    "    normalized_poses = []\n",
    "\n",
    "    for pose in frame[\"predictions\"]:\n",
    "\n",
    "        pose_scores.append(pose[\"score\"])\n",
    "        pose_coords = 0\n",
    "        for i in range(0, len(pose[\"keypoints\"]), 3):\n",
    "            if pose[\"keypoints\"][i + 2] != 0:\n",
    "                pose_coords += 1\n",
    "\n",
    "        # To find the typically small proportion of poses that are complete\n",
    "        # if pose_coords == 17:\n",
    "        #     print(frame['frame'])\n",
    "\n",
    "        pose_coords_counts.append(pose_coords)\n",
    "\n",
    "    if num_poses > 0:\n",
    "        avg_score = sum(pose_scores) / num_poses\n",
    "        avg_coords_per_pose = sum(pose_coords_counts) / num_poses\n",
    "\n",
    "    pose_series[\"avg_score\"].append(avg_score)\n",
    "    pose_series[\"avg_coords_per_pose\"].append(avg_coords_per_pose)\n",
    "\n",
    "print(\"Total frames:\", len(pose_series[\"frame\"]))\n",
    "\n",
    "print(\"Duration:\", pose_series[\"timestamp\"][len(pose_series[\"timestamp\"]) - 1].time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolox.tracker.byte_tracker import BYTETracker\n",
    "import json\n",
    "\n",
    "# This can be run as a separate preprocessing/data ingest step,\n",
    "# as it only relies on the data in the Open PifPaf detection output\n",
    "# JSON. It is used to generate an augmented version of the JSON, the\n",
    "# only difference being that detected poses that ByteTrack was able\n",
    "# to track across multiple frames are given consistent tracking IDs.\n",
    "\n",
    "# Fake tracker arguments\n",
    "class TrackerArgs:\n",
    "    def __init__(self):\n",
    "        self.track_thresh = 0.5 # Mminimum pose score for tracking -- may need to relax it...\n",
    "        self.track_buffer = 30\n",
    "        self.match_thresh = 0.8\n",
    "        self.aspect_ratio_thresh = 1.6\n",
    "        self.min_box_area = 10\n",
    "        self.mot20 = False\n",
    "\n",
    "args = TrackerArgs()\n",
    "tracker = BYTETracker(args, frame_rate=video_fps)\n",
    "\n",
    "tracking_results = []\n",
    "for frame in pose_data:\n",
    "    frameno = frame[\"frame\"] - 1 # This should always  bethe 0-based index of the frame\n",
    "    if frameno % 1000 == 0:\n",
    "        print(\"Tracking poses in frame\", frameno, \"out of\", len(pose_data))\n",
    "    # For viz purposes only\n",
    "    #frame_img = image_from_video_frame(video_file, frameno)\n",
    "    #frame_img = Image.fromarray(frame_img)\n",
    "    detections = []\n",
    "    for prediction in frame[\"predictions\"]:\n",
    "        # Need to convert prediction[\"bbox\"] to the format BYTETracker expects\n",
    "        # and package these into detections\n",
    "        # Detection format is \n",
    "        # np.array([[x1, y1, x2, y2, score] ... for all pose bboxes in frame ]) (dtype?)\n",
    "        # Actually it looks like this when it comes out of the CPU detector:\n",
    "        # tensor([[ 8.0524e+02,  2.1848e+02,  9.5338e+02,  5.8879e+02,  9.9535e-01,\n",
    "        #  9.1142e-01,  0.0000e+00], ...\n",
    "        # but then it gets converted to\n",
    "        # bboxes: [[  814.2597     224.75363    966.6578     561.90466 ] ...\n",
    "        # scores: [0.9205966 ...\n",
    "        # BYTETracker wants predictions that go minx, miny, maxx, maxy, which it then\n",
    "        # immediately converts (back) to minx, miny, width, height\n",
    "        bbox = [prediction[\"bbox\"][0], prediction[\"bbox\"][1], prediction[\"bbox\"][0] + prediction[\"bbox\"][2], prediction[\"bbox\"][1] + prediction[\"bbox\"][3]]\n",
    "        detections.append(bbox + [prediction[\"score\"]])\n",
    "    # Args 2 and 3 can differ if the image has been scaled at some point, which we're not doing\n",
    "    if len(detections):\n",
    "        online_targets = tracker.update(np.array(detections, dtype=float), [video_height, video_width], (video_height, video_width))\n",
    "        #online_tlwhs = []\n",
    "        #online_ids = []\n",
    "        #online_scores = []\n",
    "        for t in online_targets:\n",
    "            tlwh = t.tlwh\n",
    "            tid = t.track_id\n",
    "            vertical = tlwh[2] / tlwh[3] > args.aspect_ratio_thresh\n",
    "            if tlwh[2] * tlwh[3] > args.min_box_area and not vertical:\n",
    "                #online_tlwhs.append(tlwh)\n",
    "                #online_ids.append(tid)\n",
    "                #online_scores.append(t.score)\n",
    "                tracking_results.append(\n",
    "                    [frameno, tid, round(tlwh[0], 2), round(tlwh[1], 2), round(tlwh[2], 2), round(tlwh[3], 2), round(t.score, 2)]\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_info = [video_height, video_width]\n",
    "img_size = (video_height, video_width)\n",
    "img_h, img_w = img_info[0], img_info[1]\n",
    "scale = min(img_size[0] / float(img_h), img_size[1] / float(img_w))\n",
    "print(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tracking_results[0])\n",
    "frameno = tracking_results[0][0]\n",
    "for pred in pose_data[frameno]['predictions']:\n",
    "    print(pred['bbox'], pred['score'])\n",
    "    print([round(item, 2) for item in pred['bbox']], round(pred['score'], 2))\n",
    "    print(np.array(pred['bbox'], dtype=float) / float(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "tracking_matches = 0\n",
    "tracking_ids = set()\n",
    "\n",
    "# Merge tracking results with pose data\n",
    "tracked_pose_data = pose_data.copy()\n",
    "\n",
    "for res in tracking_results:\n",
    "    res_bbox = [res[2], res[3], res[4], res[5]]\n",
    "    frameno = res[0]\n",
    "    if frameno % 1000 == 0:\n",
    "        print(\"Merging tracking data in frame\", frameno, \"out of\", len(tracked_pose_data))\n",
    "    matched_predictions = []\n",
    "    # The bbox coordinates returned by the ByteTracker usually deviate by a small\n",
    "    # amount from those it receives as input. It's not clear why, but this complicates\n",
    "    # the matching process. Fortunately, the ByteTracker doesn't ever seem to modify\n",
    "    # the pose confidence scores, so we match on those first, then if there's a tie,\n",
    "    # we choose the bbox with the smallest Euclidean distance from the tracker's bbox.\n",
    "    for poseno, prediction in enumerate(pose_data[frameno]['predictions']):\n",
    "        if res[6] == round(prediction['score'], 2):\n",
    "            matched_predictions.append({'poseno': poseno, 'bbox': prediction['bbox']})\n",
    "    match_poseno = None\n",
    "    if len(matched_predictions) == 1:\n",
    "        match_poseno = matched_predictions[0]['poseno']\n",
    "    elif len(matched_predictions) > 1:\n",
    "        match_distances = {}\n",
    "        for matched_pred in matched_predictions:\n",
    "            match_distances[matched_pred['poseno']] = math.dist(matched_pred['bbox'], res_bbox)\n",
    "        match_poseno = min(match_distances, key=match_distances.get)\n",
    "    if match_poseno is not None:\n",
    "        tracked_pose_data[frameno]['predictions'][match_poseno]['tracking_id'] = res[1]\n",
    "        tracking_matches += 1\n",
    "        tracking_ids.add(res[1])\n",
    "\n",
    "print(\"Tracked\", tracking_matches, \"poses across all frames\")\n",
    "print(\"Total entities tracked\", len(tracking_ids), \"min-max\", min(tracking_ids), max(tracking_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_pose_file = f\"{fc.selected}.tracked.openpifpaf.json\"\n",
    "print(\"Writing pose data with tracking info to\", tracked_pose_file)\n",
    "json.dump(tracked_pose_data, open(tracked_pose_file, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose normalization and clustering\n",
    "\n",
    "The following two cells need to be run to enable the pose search features of the posedata explorer app.\n",
    "\n",
    "The normalization process in the first cell can take quite a while if it has never been run on a particular set of video/posedata files (~10 minutes for a full-length play). But it then caches the results in pickle (*.p) files in the same folder as the video and posedata files, meaning the cell will take a very short amount of time on every subsequent invocation for that video.\n",
    "\n",
    "When the explorer's infrastructure switches over to using a local database to store the normalized pose coordinates and other data, these normalization and indexing steps should be entirely replaced by the database ingest process for a new video/posedata corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def progress_bar(value, max=100):\n",
    "    return HTML(\n",
    "        \"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(\n",
    "            value=value, max=max\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "normalized_pose_file = pose_file.replace(\".openpifpaf.json\", \".normalized.p\")\n",
    "metadata_file = pose_file.replace(\".openpifpaf.json\", \".metadata.p\")\n",
    "\n",
    "if (os.path.isfile(normalized_pose_file)) and (os.path.isfile(metadata_file)):\n",
    "    normalized_pose_data = pickle.load(open(normalized_pose_file, \"rb\"))\n",
    "    [normalized_pose_metadata, framepose_to_seqno] = pickle.load(\n",
    "        open(metadata_file, \"rb\")\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"Computing normalized poses for comparison and clustering\")\n",
    "    print(\"This may take a while...\")\n",
    "    bar = display(progress_bar(0, len(pose_data)), display_id=True)\n",
    "\n",
    "    # For cluster analysis, each pose must be a 1D array, and all poses must be in a 1D list\n",
    "    # that includes only the pose keypoint coordinates (not the confidence scores).\n",
    "    # So we also create a parallel data structure to keep track of the frame number and numbering\n",
    "    # within the frame of each of the poses.\n",
    "    normalized_pose_data = []\n",
    "    normalized_pose_metadata = []\n",
    "\n",
    "    framepose_to_seqno = {}\n",
    "    pose_seqno = 0\n",
    "\n",
    "    for i, frame in enumerate(pose_data):\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            bar.update(progress_bar(i, len(pose_data)))\n",
    "\n",
    "        for j, pose in enumerate(frame[\"predictions\"]):\n",
    "            normalized_coords = extract_trustworthy_coords(\n",
    "                shift_normalize_rescale_pose_coords(pose)\n",
    "            )\n",
    "            normalized_pose_data.append(normalized_coords)\n",
    "            normalized_pose_metadata.append({\"frameno\": i, \"poseno\": j})\n",
    "\n",
    "            if i in framepose_to_seqno:\n",
    "                framepose_to_seqno[i][j] = pose_seqno\n",
    "            else:\n",
    "                framepose_to_seqno[i] = {j: pose_seqno}\n",
    "\n",
    "            pose_seqno += 1\n",
    "\n",
    "    pickle.dump(normalized_pose_data, open(normalized_pose_file, \"wb\"))\n",
    "    pickle.dump(\n",
    "        [normalized_pose_metadata, framepose_to_seqno], open(metadata_file, \"wb\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS indexes of the poses, for fast nearest neighbor similarity search\n",
    "\n",
    "print(\"Indexing video posedata set for similarity search\")\n",
    "\n",
    "# FAISS can't handle NaNs in the iput vectors so use -1s instead\n",
    "faiss_pose_data = [\n",
    "    tuple(np.nan_to_num(raw_pose, nan=-1).tolist()) for raw_pose in normalized_pose_data\n",
    "]\n",
    "\n",
    "# This builds an exact (flat) index based on Euclidean distance\n",
    "faiss_L2_index = faiss.IndexFlatL2(34)\n",
    "faiss_L2_input = np.array(faiss_pose_data).astype(\"float32\")\n",
    "faiss_L2_index.add(faiss_L2_input)\n",
    "\n",
    "# The below builds an exact (flat) index based on inner-product distance,\n",
    "# which is equivalent to cosine similarity when the inputs are normalized.\n",
    "# So far, its results have not been noticeably preferable to the L2\n",
    "# (Euclidean) distance-based index, but it may be useful in the future.\n",
    "# faiss_IP_index = faiss.IndexFlatIP(34)\n",
    "# faiss_IP_input = np.array(faiss_pose_data).astype('float32')\n",
    "# faiss.normalize_L2(faiss_IP_input) # Must normalize the inputs!\n",
    "# faiss_IP_index.add(faiss_IP_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: cluster analysis of normalized poses\n",
    "\n",
    "The cell below computes a K-means clustering of the poses based on the L2 (Euclidean) similarities of their normalized coordinate vectors, then calculates and visualizes the relative sizes of the clusters and the averaged armature positions of their poses.\n",
    "\n",
    "The averaged poses are visualized on blended averages of their background (source) image regions if `AVERAGE_BACKGROUNDS` is set to `True` -- although generating these plots takes quite a bit longer than plotting averaged poses with no backgrounds, due to the overhead of averaging the background images' pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL cluster analysis of the normalized poses:\n",
    "# Compute a K-means clustering of the poses based on the L2 (Euclidean) similarities\n",
    "# of their normalized coordinate vectors, then compute and visualize the sizes of\n",
    "# the clusters and the averages of their poses.\n",
    "NUMBER_OF_CLUSTERS = 100\n",
    "IMAGE_SAMPLE = 100  # Only contribute 1 in 100 images to the background average\n",
    "AVERAGE_BACKGROUNDS = True  # Set to True to average the pose backgrounds\n",
    "\n",
    "print(f\"Clustering video poses into {NUMBER_OF_CLUSTERS} clusters\")\n",
    "kmeans_faiss = faiss.Kmeans(d=faiss_L2_input.shape[1], k=NUMBER_OF_CLUSTERS, niter=100)\n",
    "kmeans_faiss.train(faiss_L2_input)\n",
    "_, cluster_labels = kmeans_faiss.index.search(faiss_L2_input, 1)\n",
    "cluster_labels = np.array(cluster_labels).flatten()\n",
    "\n",
    "bin_counts = {}\n",
    "cluster_to_pose = {}\n",
    "\n",
    "for i in range(len(cluster_labels)):\n",
    "    ct = cluster_labels[i]\n",
    "    if ct not in bin_counts:\n",
    "        bin_counts[ct] = 1\n",
    "    else:\n",
    "        bin_counts[ct] += 1\n",
    "    if ct not in cluster_to_pose:\n",
    "        cluster_to_pose[ct] = [i]\n",
    "    else:\n",
    "        cluster_to_pose[ct].append(i)\n",
    "\n",
    "sorted_bin_counts = dict(\n",
    "    sorted(bin_counts.items(), key=lambda item: item[1], reverse=True)\n",
    ")\n",
    "sorted_bin_counts_list = list(sorted_bin_counts.values())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(sorted_bin_counts_list)), sorted_bin_counts_list)\n",
    "plt.xlabel(\"cluster\")\n",
    "plt.ylabel(\"# poses\")\n",
    "plt.show()\n",
    "\n",
    "for k in list(sorted_bin_counts.keys())[:10]:\n",
    "    cluster_poses = []\n",
    "    images_array = []\n",
    "\n",
    "    # Use some matplotlib weirdness to draw the stick figures in higher resolution\n",
    "    # but with the same axis labels (0-100 \"pixels\")\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(UPSCALE * 100 / fig.dpi, UPSCALE * 100 / fig.dpi)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    print(\"Cluster:\", k, \"| total poses:\", len(cluster_to_pose[k]))\n",
    "\n",
    "    for i, pose_id in enumerate(cluster_to_pose[k]):\n",
    "\n",
    "        cluster_poses.append(normalized_pose_data[pose_id])\n",
    "\n",
    "        # Don't average the background of every pose in the cluster,\n",
    "        # because that usually takes way too long\n",
    "        if AVERAGE_BACKGROUNDS and (i % IMAGE_SAMPLE) == 0:\n",
    "            # Get the original posedata for the pose in order to extract the background image\n",
    "            pose_frameno = normalized_pose_metadata[pose_id][\"frameno\"]\n",
    "            poseno = normalized_pose_metadata[pose_id][\"poseno\"]\n",
    "            pose_pred = pose_data[pose_frameno][\"predictions\"][poseno]\n",
    "\n",
    "            pose_base_image = extract_pose_background(\n",
    "                pose_pred, video_file, pose_frameno\n",
    "            )\n",
    "\n",
    "            # Resize/normalize the cutout background dimensions, just as is done\n",
    "            # for the pose itself\n",
    "            resized_image = cv2.resize(\n",
    "                pose_base_image,\n",
    "                dsize=(POSE_MAX_DIM * UPSCALE, POSE_MAX_DIM * UPSCALE),\n",
    "                interpolation=cv2.INTER_LANCZOS4,\n",
    "            )\n",
    "            images_array.append(resized_image)\n",
    "\n",
    "    if AVERAGE_BACKGROUNDS:\n",
    "        images_array = np.array(images_array, dtype=float)\n",
    "\n",
    "        # Average the RGB values of all of the pose background images\n",
    "        avg_background_img = np.mean(images_array, axis=0).astype(np.uint8)\n",
    "        plt.imshow(avg_background_img)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(action=\"ignore\", message=\"Mean of empty slice\")\n",
    "        cluster_average = np.nanmean(np.array(cluster_poses), axis=0).tolist()\n",
    "\n",
    "    armature_prevalences = get_armature_prevalences(cluster_poses)\n",
    "    cluster_average = np.array_split(cluster_average, len(cluster_average) / 2)\n",
    "    cluster_average_img = draw_normalized_and_unflattened_pose(\n",
    "        cluster_average, armature_prevalences=armature_prevalences\n",
    "    )\n",
    "\n",
    "    plt.imshow(cluster_average_img)\n",
    "    axis_labels = [0] + list(range(0, 100, 20))\n",
    "    axis_label_locs = [lab * UPSCALE for lab in axis_labels]\n",
    "\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(axis_label_locs))\n",
    "    ax.set_xticklabels(axis_labels)\n",
    "    ax.yaxis.set_major_locator(mticker.FixedLocator(axis_label_locs))\n",
    "    ax.set_yticklabels(axis_labels)\n",
    "    plt.show()\n",
    "\n",
    "    # If we want to inspect some of the poses in the cluster\n",
    "    # for i in range (10):\n",
    "    #     this_pose = np.array_split(cluster_poses[i], len(cluster_poses[i]) / 2)\n",
    "    #     pose_img =  draw_normalized_and_unflattened_pose(this_pose)\n",
    "\n",
    "    #     plt.imshow(pose_img)\n",
    "    #     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and launch the explorer app\n",
    "\n",
    "This displays an interactive chart visualization of the attributes of the posedata in the .json output file across the runtime of the video.\n",
    "\n",
    "Clicking anywhere in the chart, moving the slider, or clicking the prev/next buttons will select a frame and draw the poses detected in that frame, with the option of displaying the actual image from the source video as the \"background.\" When a frame is selected, it is also possible to click a specific pose in the frame window to select that pose for comparison with a second pose (which is also selected by clicking on it). And the first selected pose can be used as the \"query\" to search for the most similar poses across the entire video, which can then be viewed and paged through.\n",
    "\n",
    "Please see the cell below the next if you are running this notebook in VS Code. Note also that the Jupyter server must be running on port 8888 (or 8889) for the explorer app to work in Jupyter/JupterLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_bokeh_image(pil_img, target_width, target_height):\n",
    "    \"\"\"The Bokeh interactive notebook tools will only display image data if it's formatted in a particular way\"\"\"\n",
    "    img_array = np.array(pil_img.transpose(Image.Transpose.FLIP_TOP_BOTTOM))\n",
    "\n",
    "    img = np.empty(img_array.shape[:2], dtype=np.uint32)\n",
    "    view = img.view(dtype=np.uint8).reshape(img_array.shape)\n",
    "\n",
    "    for i in range(target_height):\n",
    "        for j in range(target_width):\n",
    "            view[i, j, 0] = img_array[i, j, 0]\n",
    "            view[i, j, 1] = img_array[i, j, 1]\n",
    "            view[i, j, 2] = img_array[i, j, 2]\n",
    "            view[i, j, 3] = img_array[i, j, 3]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def bkapp(doc):\n",
    "    \"\"\"Define and run the Bokeh interactive notebook (Python + Javascript) application\"\"\"\n",
    "\n",
    "    # Some session data is best stored in a global dictionary\n",
    "    data = {}\n",
    "\n",
    "    max_y = max(pose_series[\"avg_coords_per_pose\"] + pose_series[\"num_poses\"])\n",
    "\n",
    "    # This is the main interactive timeline chart\n",
    "    tl = figure(\n",
    "        width=FIGURE_WIDTH,\n",
    "        height=FIGURE_HEIGHT,\n",
    "        title=video_file,\n",
    "        min_border=10,\n",
    "        y_range=(0, max_y + 1),\n",
    "        tools=\"save,box_zoom,pan,reset\",\n",
    "    )\n",
    "    # Format the X axis as hour-minute-second timecodes\n",
    "    tl.x_range = Range1d(min(pose_series[\"timestamp\"]), max(pose_series[\"timestamp\"]))\n",
    "    tl.xaxis.axis_label = \"Time\"\n",
    "    time_formatter = DatetimeTickFormatter(\n",
    "        hourmin=\"%H:%M:%S\",\n",
    "        minutes=\"%H:%M:%S\",\n",
    "        minsec=\"%H:%M:%S\",\n",
    "        seconds=\"%Ss\",\n",
    "        milliseconds=\"%3Nms\",\n",
    "    )\n",
    "    # The 3 main pose-related time series to be visualized on the timeline\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"num_poses\"],\n",
    "        legend_label=\"Poses per frame\",\n",
    "        color=\"blue\",\n",
    "        alpha=0.6,\n",
    "        line_width=2,\n",
    "    )\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"avg_coords_per_pose\"],\n",
    "        legend_label=\"Coords per pose\",\n",
    "        color=\"red\",\n",
    "        alpha=0.6,\n",
    "        line_width=2,\n",
    "    )\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        [0] * len(pose_series[\"frame\"]),\n",
    "        color=\"orange\",\n",
    "        alpha=0,\n",
    "        line_width=2,\n",
    "        name=\"similar_poses\",\n",
    "    )\n",
    "\n",
    "    # The left Y axis corresponds to counts of poses and coordinates\n",
    "    tl.yaxis.axis_label = \"Poses or Coords\"\n",
    "    tl.extra_y_ranges = {\"avg_score\": Range1d(0, 1)}\n",
    "    tl.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"avg_score\"],\n",
    "        y_range_name=\"avg_score\",\n",
    "        legend_label=\"Avg pose score\",\n",
    "        color=\"green\",\n",
    "        alpha=0.4,\n",
    "        line_width=2,\n",
    "    )\n",
    "    # The right Y axis corresponds to the average pose score (from 0 to 1), and to the\n",
    "    # per-frame similarity score (0 to 1) when a pose search query has been run.\n",
    "    tl.add_layout(\n",
    "        LinearAxis(\n",
    "            y_range_name=\"avg_score\", axis_label=\"Avg Pose Score or Cosine Similarity\"\n",
    "        ),\n",
    "        \"right\",\n",
    "    )\n",
    "    tl.xaxis.formatter = time_formatter\n",
    "    tl.xaxis.ticker.desired_num_ticks = 10\n",
    "    tl.legend.click_policy = \"hide\"\n",
    "    frame_line = Span(\n",
    "        location=pose_series[\"timestamp\"][0],\n",
    "        dimension=\"height\",\n",
    "        line_color=\"red\",\n",
    "        line_width=3,\n",
    "    )\n",
    "    tl.add_layout(frame_line)\n",
    "\n",
    "    def tl_tap(event):\n",
    "        \"\"\"When the chart is clicked, move the slider to the appropriate frame\"\"\"\n",
    "        # Do not respond to clicks on the top 25% of the plot. This is to try inadvertently\n",
    "        # selecting a new frame when the user just wants to click on the legend to hide or\n",
    "        # show one of the time-series glyphs.\n",
    "        if event.y > 0.75 * max_y:\n",
    "            return\n",
    "        # event.x is a timestamp, so it needs to be converted to a frameno\n",
    "        start_dt = datetime(1900, 1, 1)\n",
    "        dt = datetime.utcfromtimestamp(event.x / 1000)\n",
    "        t_delta = dt - start_dt\n",
    "        clicked_frame = round(t_delta.total_seconds() * video_fps)\n",
    "        slider_callback(None, slider.value, clicked_frame)\n",
    "\n",
    "    tl_tap_tool = TapTool()\n",
    "    tl_crosshair_tool = CrosshairTool()\n",
    "\n",
    "    def get_frame_info(fn):\n",
    "        return f\"Frame info: {pose_series['num_poses'][fn]} detected poses, {pose_series['avg_coords_per_pose'][fn]:.3f} avg coords/pose, {pose_series['avg_score'][fn]:.3f} avg pose score\"\n",
    "\n",
    "    info_div = Div(text=get_frame_info(0))\n",
    "\n",
    "    tl.add_tools(tl_tap_tool, tl_crosshair_tool)\n",
    "    tl.on_event(\"tap\", tl_tap)\n",
    "\n",
    "    # This is the second figure, where the poses in the selected frame are drawn\n",
    "    fr = figure(\n",
    "        x_range=(0, video_width),\n",
    "        y_range=(0, video_height),\n",
    "        width=FIGURE_WIDTH,\n",
    "        height=int(FIGURE_WIDTH / video_width * video_height),\n",
    "        title=\"Poses in selected frame\",\n",
    "        tools=\"save\",\n",
    "    )\n",
    "    # Add an invisible glyph to suppress the \"figure has no renderers\" warning\n",
    "    fr.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    pose_info_div = Div(text=\"Click to poses to compare\")\n",
    "\n",
    "    # This is the drawing of the first pose selected from a frame\n",
    "    pose_p1 = figure(\n",
    "        x_range=(0, POSE_MAX_DIM),\n",
    "        y_range=(0, POSE_MAX_DIM),\n",
    "        width=POSE_MAX_DIM * 2,\n",
    "        height=POSE_MAX_DIM * 2,\n",
    "        title=\"\",\n",
    "        tools=\"\",\n",
    "    )\n",
    "    # Add an invisible glyph to suppress the \"figure has no renderers\" warning\n",
    "    pose_p1.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    # This is the second pose selected from a frame\n",
    "    pose_p2 = figure(\n",
    "        x_range=(0, POSE_MAX_DIM),\n",
    "        y_range=(0, POSE_MAX_DIM),\n",
    "        width=POSE_MAX_DIM * 2,\n",
    "        height=POSE_MAX_DIM * 2,\n",
    "        title=\"\",\n",
    "        tools=\"\",\n",
    "    )\n",
    "    # Add an invisible glyph to suppress the \"figure has no renderers\" warning\n",
    "    pose_p2.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    def background_toggle_handler(event):\n",
    "        \"\"\"When the image underlay is toggled on or off, prompt the slider to redraw the frame\"\"\"\n",
    "        slider_callback(None, slider.value, slider.value)\n",
    "\n",
    "    background_switch = Toggle(label=\"show background\", active=False)\n",
    "    background_switch.on_click(background_toggle_handler)\n",
    "\n",
    "    def slider_callback(attr, old, new):\n",
    "        \"\"\"\n",
    "        When the slider moves, draw the poses in the new frame and show the background if desired.\n",
    "        Also erase the selected pose drawings and the search results (not sure this is desirable).\n",
    "        \"\"\"\n",
    "        slider.value = new\n",
    "        fr.renderers = []\n",
    "        if background_switch.active:\n",
    "            rgba_bg = image_from_video_frame(video_file, new)\n",
    "            pil_bg = Image.fromarray(rgba_bg)\n",
    "            frame_img = draw_frame(pose_data[new], video_width, video_height, pil_bg)\n",
    "        else:\n",
    "            frame_img = draw_frame(pose_data[new], video_width, video_height)\n",
    "        img = pil_to_bokeh_image(frame_img, video_width, video_height)\n",
    "        fr.image_rgba(image=[img], x=0, y=0, dw=img.shape[1], dh=img.shape[0])\n",
    "        if old != new:\n",
    "            info_div.text = get_frame_info(new)\n",
    "            frame_line.location = pose_series[\"timestamp\"][new]\n",
    "            pose_p1.title.text = \"\"\n",
    "            pose_p1.renderers = []\n",
    "            pose_p2.title.text = \"\"\n",
    "            pose_p2.renderers = []\n",
    "            pose_info_div.text = \"Click two poses to compare\"\n",
    "            for pose_box in similar_poses:\n",
    "                pose_box.renderers = []\n",
    "\n",
    "    slider = Slider(\n",
    "        start=0, end=len(pose_data) - 1, value=0, step=1, title=\"Selected frame\"\n",
    "    )\n",
    "    slider.on_change(\"value_throttled\", slider_callback)\n",
    "\n",
    "    def get_pose_extent_maps(frameno):\n",
    "        \"\"\"\n",
    "        Determine the regions around each pose drawn on the frame that can be clicked\n",
    "        to select them.\n",
    "        \"\"\"\n",
    "        pose_extent_maps = []\n",
    "        for i, pose_prediction in enumerate(pose_data[frameno][\"predictions\"]):\n",
    "\n",
    "            if \"bbox\" in pose_prediction:\n",
    "                bbox = pose_prediction[\"bbox\"]\n",
    "            else:\n",
    "                extent = get_pose_extent(pose_prediction)\n",
    "                bbox = [\n",
    "                    extent[0],\n",
    "                    extent[1],\n",
    "                    extent[2] - extent[0],\n",
    "                    extent[3] - extent[1],\n",
    "                ]\n",
    "\n",
    "            extent_map = {\n",
    "                \"poseno\": i,\n",
    "                \"min_x\": bbox[0],\n",
    "                \"min_y\": video_height - bbox[3] - bbox[1],\n",
    "                \"max_x\": bbox[0] + bbox[2],\n",
    "                \"max_y\": video_height - bbox[1],\n",
    "            }\n",
    "\n",
    "            pose_extent_maps.append(extent_map)\n",
    "\n",
    "        return pose_extent_maps\n",
    "\n",
    "    def match_pose_pixel_maps(x, y, pose_extent_maps):\n",
    "        \"\"\"\n",
    "        When an x,y coordinate on the frame is clicked, check the regions calculated in\n",
    "        get_pose_extent_maps() to see if the user wants to select one (or more) of the\n",
    "        poses in the frame.\n",
    "        \"\"\"\n",
    "        matched_poses = []\n",
    "        for extent_map in pose_extent_maps:\n",
    "            if (\n",
    "                x >= extent_map[\"min_x\"]\n",
    "                and x <= extent_map[\"max_x\"]\n",
    "                and y >= extent_map[\"min_y\"]\n",
    "                and y <= extent_map[\"max_y\"]\n",
    "            ):\n",
    "                matched_poses.append(extent_map[\"poseno\"])\n",
    "        return matched_poses\n",
    "\n",
    "    def fr_tap(event):\n",
    "        \"\"\"\n",
    "        When the frame is clicked, determine if one of the poses in the frame has\n",
    "        been selected, then draw it in one of the two boxes below (if available)\n",
    "        and, if there are now two poses drawn in the boxes below, calculate and\n",
    "        display their cosine similarity score.\n",
    "        \"\"\"\n",
    "        pixel_key = f\"{int(event.x)}, {int(event.y)}\"\n",
    "        pose_extent_maps = get_pose_extent_maps(slider.value)\n",
    "        clicked_poses = match_pose_pixel_maps(event.x, event.y, pose_extent_maps)\n",
    "        if len(clicked_poses):\n",
    "            pose_img = normalize_and_draw_pose(\n",
    "                pose_data[slider.value][\"predictions\"][clicked_poses[0]],\n",
    "                video_file\n",
    "            )\n",
    "            pose_img = pil_to_bokeh_image(pose_img, POSE_MAX_DIM, POSE_MAX_DIM)\n",
    "\n",
    "            if pose_p1.title.text == \"\":\n",
    "                pose_p1.image_rgba(\n",
    "                    image=[pose_img],\n",
    "                    x=0,\n",
    "                    y=0,\n",
    "                    dw=pose_img.shape[1],\n",
    "                    dh=pose_img.shape[0],\n",
    "                )\n",
    "                pose_p1.title = f\"{clicked_poses[0]+1}\"\n",
    "                pose_info_div.text = \"Please click another pose for comparison\"\n",
    "            elif pose_p1.title.text != \"\" and pose_p2.title.text == \"\":\n",
    "                pose_p2.image_rgba(\n",
    "                    image=[pose_img],\n",
    "                    x=0,\n",
    "                    y=0,\n",
    "                    dw=pose_img.shape[1],\n",
    "                    dh=pose_img.shape[0],\n",
    "                )\n",
    "                pose_p2.title = f\"{clicked_poses[0]+1}\"\n",
    "\n",
    "                normalized_p1 = shift_normalize_rescale_pose_coords(\n",
    "                    pose_data[slider.value][\"predictions\"][int(pose_p1.title.text) - 1]\n",
    "                )\n",
    "                normalized_p2 = shift_normalize_rescale_pose_coords(\n",
    "                    pose_data[slider.value][\"predictions\"][int(pose_p2.title.text) - 1]\n",
    "                )\n",
    "\n",
    "                cosine_similarity = compare_poses_cosine(\n",
    "                    normalized_p1,\n",
    "                    normalized_p2,\n",
    "                )\n",
    "                p1_angles = compute_joint_angles(normalized_p1)\n",
    "                p2_angles = compute_joint_angles(normalized_p2)\n",
    "                pose_p2.title = f\"{clicked_poses[0]+1}\"\n",
    "                angle_similarity = compare_poses_angles(p1_angles, p2_angles)\n",
    "                pose_info_div.text = f\"Cosine similarity between pose keypoints: {(cosine_similarity*100):3.3f}% | Similarity between pose joint angles: {(angle_similarity*100):3.3f}%\"\n",
    "\n",
    "    fr_tap_tool = TapTool()\n",
    "\n",
    "    fr.add_tools(fr_tap_tool)\n",
    "    fr.on_event(\"tap\", fr_tap)\n",
    "\n",
    "    # Buttons to advance or back up the frame selector slider by one frame\n",
    "    def prev_handler(event):\n",
    "        slider_callback(None, slider.value, max(0, slider.value - 1))\n",
    "\n",
    "    def next_handler(event):\n",
    "        slider_callback(None, slider.value, min(slider.value + 1, len(pose_data) - 1))\n",
    "\n",
    "    prev_button = Button(label=\"prev\")\n",
    "    prev_button.on_click(prev_handler)\n",
    "    next_button = Button(label=\"next\")\n",
    "    next_button.on_click(next_handler)\n",
    "\n",
    "    search_info_div = Div(text=\"L2 (Euclidean distance) similar pose search\")\n",
    "\n",
    "    SIMILAR_POSES_TO_SHOW = 4\n",
    "    SIMILAR_MATCHES_TO_FIND = 1000\n",
    "    POSE_SIMILARITY_THRESHOLD = 0.8\n",
    "    similar_poses = []\n",
    "\n",
    "    for s in range(SIMILAR_POSES_TO_SHOW):\n",
    "        similar_poses.append(\n",
    "            figure(\n",
    "                x_range=(0, POSE_MAX_DIM),\n",
    "                y_range=(0, POSE_MAX_DIM),\n",
    "                width=POSE_MAX_DIM * 2,\n",
    "                height=POSE_MAX_DIM * 2,\n",
    "                title=\"\",\n",
    "                tools=\"\",\n",
    "            )\n",
    "        )\n",
    "    for pose_box in similar_poses:\n",
    "        pose_box.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    # Need to kep track of match data for paging through all of the results\n",
    "    data[\"match_indices\"] = None\n",
    "    data[\"valid_search_results\"] = 0\n",
    "    data[\"search_results_index\"] = 0\n",
    "    similar_frame_scores = [0] * len(pose_series[\"frame\"])\n",
    "    match_cosine_similarities = {}\n",
    "    target_frameno = None\n",
    "    target_poseno = None\n",
    "\n",
    "    def draw_similar_poses(start_rank):\n",
    "        \"\"\"\n",
    "        Draw up to SIMILAR_POSES_TO_SHOW in boxes below the search/query info div and\n",
    "        search results paging back/forward buttons.\n",
    "        \"\"\"\n",
    "        # Clear any previously drawn poses\n",
    "        for pose_box in similar_poses:\n",
    "            pose_box.renderers = []\n",
    "\n",
    "        matches_to_show = 0\n",
    "        data[\"search_results_index\"] = start_rank\n",
    "\n",
    "        match_framenos = []\n",
    "        match_scores = []\n",
    "        match_timecodes = []\n",
    "        matches_advanced = 0\n",
    "\n",
    "        while matches_to_show < SIMILAR_POSES_TO_SHOW:\n",
    "\n",
    "            current_match_rank = matches_advanced + start_rank\n",
    "            matches_advanced += 1\n",
    "\n",
    "            match_index = data[\"match_indices\"][current_match_rank]\n",
    "            match_frameno = normalized_pose_metadata[match_index][\"frameno\"]\n",
    "            match_poseno = normalized_pose_metadata[match_index][\"poseno\"]\n",
    "\n",
    "            # Skip the query pose if it's returned as a (100%) match. This is *usually*\n",
    "            # the highest-ranked match, but may not always be so, depending on the indexing\n",
    "            # process.\n",
    "            if target_frameno == match_frameno and target_poseno == match_poseno:\n",
    "                continue\n",
    "\n",
    "            matches_to_show += 1\n",
    "\n",
    "            match_framenos.append(str(match_frameno))\n",
    "            match_scores.append(f\"{match_cosine_similarities[match_index]*100:3.3f}%\")\n",
    "            match_timecodes.append(\n",
    "                str(timedelta(seconds=pose_series[\"seconds\"][match_frameno])).replace(\n",
    "                    \"0000\", \"\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if background_switch.active:\n",
    "                match_img = normalize_and_draw_pose(\n",
    "                    pose_data[match_frameno][\"predictions\"][match_poseno],\n",
    "                    video_file,\n",
    "                    match_frameno\n",
    "                )\n",
    "            else:\n",
    "                match_img = normalize_and_draw_pose(\n",
    "                    pose_data[match_frameno][\"predictions\"][match_poseno],\n",
    "                    video_file\n",
    "                )\n",
    "\n",
    "            match_img = pil_to_bokeh_image(match_img, POSE_MAX_DIM, POSE_MAX_DIM)\n",
    "\n",
    "            similar_poses[matches_to_show - 1].image_rgba(\n",
    "                image=[match_img],\n",
    "                x=0,\n",
    "                y=0,\n",
    "                dw=match_img.shape[1],\n",
    "                dh=match_img.shape[0],\n",
    "            )\n",
    "\n",
    "        search_info_div.text = f\"matches in frames {', '.join(match_framenos)} | {', '.join(match_timecodes)} | scores {', '.join(match_scores)}\"\n",
    "\n",
    "    def find_similar_poses():\n",
    "        \"\"\"\n",
    "        Run the query against the FAISS pose vector index to find the SIMILAR_MATCHES_TO_FIND\n",
    "        most similar poses to the query pose (the first pose in the comparison boxes) whose\n",
    "        calculated cosine similarities are above the POSE_SIMILARITY_THRESHOLD. Note that the\n",
    "        cosine similarity is (for now) calculated on the fly and used for the thresholding,\n",
    "        rather than the similarity scores returned by the FAISS index (these determine the\n",
    "        order in which the search results are ranked, but are relatively more difficult to\n",
    "        interpret and to present to the user in an intuitive manner).\n",
    "        \"\"\"\n",
    "        if pose_p1.title.text == \"\":\n",
    "            return\n",
    "\n",
    "        for pose_box in similar_poses:\n",
    "            pose_box.renderers = []\n",
    "\n",
    "        target_frameno = slider.value\n",
    "        target_poseno = int(pose_p1.title.text) - 1\n",
    "\n",
    "        target_pose_w_confs = shift_normalize_rescale_pose_coords(\n",
    "            pose_data[target_frameno][\"predictions\"][target_poseno]\n",
    "        )\n",
    "        target_pose = extract_trustworthy_coords(target_pose_w_confs)\n",
    "\n",
    "        target_pose_query = np.array([np.nan_to_num(target_pose, nan=-1)]).astype(\n",
    "            \"float32\"\n",
    "        )\n",
    "\n",
    "        D, I = faiss_L2_index.search(target_pose_query, SIMILAR_MATCHES_TO_FIND)\n",
    "\n",
    "        data[\"match_indices\"] = I[0]\n",
    "        data[\"valid_search_results\"] = 0\n",
    "        data[\"search_results_index\"] = 0\n",
    "        similar_frame_scores = [0] * len(pose_series[\"frame\"])\n",
    "\n",
    "        for m in range(SIMILAR_MATCHES_TO_FIND):\n",
    "            match_index = data[\"match_indices\"][m]\n",
    "            if match_index != -1:\n",
    "                match_frameno = normalized_pose_metadata[match_index][\"frameno\"]\n",
    "                match_poseno = normalized_pose_metadata[match_index][\"poseno\"]\n",
    "\n",
    "                cosine_similarity = compare_poses_cosine(\n",
    "                    target_pose_w_confs,\n",
    "                    shift_normalize_rescale_pose_coords(\n",
    "                        pose_data[match_frameno][\"predictions\"][match_poseno]\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                match_cosine_similarities[match_index] = cosine_similarity\n",
    "\n",
    "                if cosine_similarity >= POSE_SIMILARITY_THRESHOLD:\n",
    "                    if similar_frame_scores[match_frameno] > 0:\n",
    "                        similar_frame_scores[match_frameno] = max(\n",
    "                            similar_frame_scores[match_frameno], cosine_similarity\n",
    "                        )\n",
    "                    else:\n",
    "                        similar_frame_scores[match_frameno] = cosine_similarity\n",
    "\n",
    "                    data[\"valid_search_results\"] += 1\n",
    "\n",
    "        similar_poses_renderers = tl.select(name=\"similar_poses\")\n",
    "        if len(similar_poses_renderers) > 0:\n",
    "            for sim_pose_renderer in similar_poses_renderers:\n",
    "                try:\n",
    "                    tl.renderers.remove(sim_pose_renderer)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "        # Mark the frames on the video posedata timeline that have high match scores\n",
    "        tl.line(\n",
    "            pose_series[\"timestamp\"],\n",
    "            similar_frame_scores,\n",
    "            y_range_name=\"avg_score\",\n",
    "            legend_label=\"Similar poses\",\n",
    "            color=\"orange\",\n",
    "            alpha=0.8,\n",
    "            line_width=2,\n",
    "            name=\"similar_poses\",\n",
    "        )\n",
    "\n",
    "    def find_and_draw_similar_poses():\n",
    "        find_similar_poses()\n",
    "        draw_similar_poses(0)\n",
    "\n",
    "    def get_similar_poses_handler(event):\n",
    "        # Need to add a tick callback to display the \"please wait\" message\n",
    "        search_info_div.text = (\n",
    "            \"<strong>Searching for similar poses, please wait...</strong>\"\n",
    "        )\n",
    "        doc.add_next_tick_callback(find_and_draw_similar_poses)\n",
    "\n",
    "    def reset_subposes_handler(event):\n",
    "        \"\"\"This clears both the two similarity/query pose boxes as well as the match boxes.\"\"\"\n",
    "        pose_p1.title.text = \"\"\n",
    "        pose_p1.renderers = []\n",
    "        pose_p2.title.text = \"\"\n",
    "        pose_p2.renderers = []\n",
    "        pose_info_div.text = \"\"\n",
    "        for pose_box in similar_poses:\n",
    "            pose_box.renderers = []\n",
    "        similar_poses_renderers = tl.select(name=\"similar_poses\")\n",
    "        if len(similar_poses_renderers) > 0:\n",
    "            for sim_pose_renderer in similar_poses_renderers:\n",
    "                try:\n",
    "                    tl.renderers.remove(sim_pose_renderer)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        for li in tl.legend.items:\n",
    "            if li.label[\"value\"] == \"Similar poses\":\n",
    "                li.visible = False\n",
    "        search_info_div.text = \"L2 (Euclidean distance) similar pose search\"\n",
    "\n",
    "    reset_subposes_button = Button(label=\"clear\")\n",
    "    reset_subposes_button.on_click(reset_subposes_handler)\n",
    "\n",
    "    get_similar_poses_button = Button(label=\"look up 1st pose\")\n",
    "    get_similar_poses_button.on_click(get_similar_poses_handler)\n",
    "\n",
    "    frame_control_row = row(children=[prev_button, next_button, background_switch])\n",
    "\n",
    "    pose_buttons_column = column(reset_subposes_button, get_similar_poses_button)\n",
    "\n",
    "    subposes_row = row(children=[pose_p1, pose_p2, pose_buttons_column])\n",
    "\n",
    "    # Buttons to page through the search matches in groups of SIMILAR_POSES_TO_SHOW\n",
    "    def prev_similar_poses_handler(event):\n",
    "        draw_similar_poses(max(0, data[\"search_results_index\"] - SIMILAR_POSES_TO_SHOW))\n",
    "\n",
    "    def next_similar_poses_handler(event):\n",
    "        draw_similar_poses(\n",
    "            min(\n",
    "                data[\"search_results_index\"] + SIMILAR_POSES_TO_SHOW,\n",
    "                data[\"valid_search_results\"] - SIMILAR_POSES_TO_SHOW,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    prev_similar_button = Button(label=\"previous group of poses\")\n",
    "    prev_similar_button.on_click(prev_similar_poses_handler)\n",
    "\n",
    "    next_similar_button = Button(label=\"next group of poses\")\n",
    "    next_similar_button.on_click(next_similar_poses_handler)\n",
    "\n",
    "    similar_poses_controls = row(children=[prev_similar_button, next_similar_button])\n",
    "\n",
    "    similar_poses_row = row(children=similar_poses)\n",
    "\n",
    "    layout_column = column(\n",
    "        tl,\n",
    "        slider,\n",
    "        info_div,\n",
    "        frame_control_row,\n",
    "        fr,\n",
    "        pose_info_div,\n",
    "        subposes_row,\n",
    "        search_info_div,\n",
    "        similar_poses_controls,\n",
    "        similar_poses_row,\n",
    "    )\n",
    "\n",
    "    doc.add_root(layout_column)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the notebook in VS Code:** As of early 2023, if you are running this notebook in VS Code instead of Jupyter or JupyterLab, the cell below will not work (BokehJS will load, but no figures will appear) without the following workaround:\n",
    "\n",
    "Take note of the error message that appears when you try to run the cell above, particularly the long alphanumeric string suggested as a value for `BOKEH_ALLOW_WS_ORIGIN`. Copy this string, then uncomment the last two lines in the cell below, paste the alphanumeric string in place of the `INSERT_BOKEH_ALLOW_WS_ORIGIN_VALUE_HERE` text, run the cell, then try running the cell above to launch the explorer app again. It should work now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are following the steps above to run the explorer app in VS Code,\n",
    "# uncomment the following lines (remove the '#'s) before running this cell:\n",
    "# import os\n",
    "# os.environ[\"BOKEH_ALLOW_WS_ORIGIN\"] = \"0jubpfudr7ckf8qfh6dong6lr67pqrvbr5ugu8db8kcm4g6se70e\"\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "show(bkapp, notebook_url=\"localhost:8888\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of frame-by-frame pose drawing\n",
    "\n",
    "The cell below uses a different viz library to draw the poses in each successive frame on an HTML canvas, at the same frame rate as the source video.\n",
    "\n",
    "Note that this drawing library (`ipycanvas`) doesn't play well with the Bokeh interactive application above, which is why the somewhat clunkier PIL ImageDraw library is used to draw the poses there instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "canvas = Canvas(width=video_width, height=video_height, sync_image_data=True)\n",
    "\n",
    "display(canvas)\n",
    "\n",
    "\n",
    "def draw_frame_on_canvas(frame, canvas):\n",
    "\n",
    "    for pose_prediction in frame[\"predictions\"]:\n",
    "        pose_coords = np.array_split(\n",
    "            pose_prediction[\"keypoints\"], len(pose_prediction[\"keypoints\"]) / 3\n",
    "        )\n",
    "\n",
    "        for i, seg in enumerate(OPP_COCO_SKELETON):\n",
    "\n",
    "            if pose_coords[seg[0] - 1][2] == 0 or pose_coords[seg[1] - 1][2] == 0:\n",
    "                continue\n",
    "\n",
    "            canvas.stroke_style = OPP_COCO_COLORS[i]\n",
    "            canvas.line_width = 2\n",
    "\n",
    "            canvas.stroke_line(\n",
    "                pose_coords[seg[0] - 1][0],\n",
    "                pose_coords[seg[0] - 1][1],\n",
    "                pose_coords[seg[1] - 1][0],\n",
    "                pose_coords[seg[1] - 1][1],\n",
    "            )\n",
    "\n",
    "\n",
    "# This will \"animate\" all of the detected poses starting from the beginning of the video\n",
    "for frame in pose_data:\n",
    "\n",
    "    with hold_canvas():\n",
    "\n",
    "        canvas.clear()\n",
    "\n",
    "        draw_frame_on_canvas(frame, canvas)\n",
    "\n",
    "        sleep(1 / video_fps)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
