{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A notebook for exploring video posedata\n",
    "\n",
    "**Intended use:** the user selects a video that is accompanied by already extracted posedata in a .json file. The notebook provides visualizations that summarize the quality and content of the poses extracted across all frames of the video, as well as armature plots of the detected poses in a selected frame. These can be viewed separately from the source video and even animated.\n",
    "\n",
    "Note that at present, this only works with .json output files generated via the Open PifPaf command-line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import (\n",
    "    DatetimeTickFormatter,\n",
    "    Range1d,\n",
    "    LinearAxis,\n",
    "    Slider,\n",
    "    Div,\n",
    "    TapTool,\n",
    "    CrosshairTool,\n",
    "    Button,\n",
    "    Toggle,\n",
    ")\n",
    "from bokeh.models.widgets.inputs import Select\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.themes import Theme\n",
    "import cv2\n",
    "from IPython.display import HTML, display\n",
    "from ipywidgets import Dropdown, Layout\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and display the video/posedata selector widget\n",
    "\n",
    "**Important:** for a video to appear in the dropdown menu, the video and its posedata output file must be present at the path specified in `source_data_folder`, which is by default the folder containing this notebook. The names of the matched video and posedata files should be identical, other than that the posedata file will have .openpifpaf.json appended to the name of the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_folder = Path.cwd()\n",
    "\n",
    "\n",
    "def get_available_videos(data_folder):\n",
    "    \"\"\"Available videos will be limited to those with a .json and matching video (.mp4, .avi, etc)\n",
    "    file in a predefined directory (the notebook's running directory, for now)\"\"\"\n",
    "    available_json_files = list(data_folder.glob(\"*.json\"))\n",
    "    available_video_files = (\n",
    "        p.resolve()\n",
    "        for p in Path(data_folder).glob(\"*\")\n",
    "        if p.suffix in {\".avi\", \".mp4\", \".mov\", \".mkv\", \".webm\"}\n",
    "        and \"openpifpaf\" not in p.name\n",
    "    )\n",
    "    available_json = [\n",
    "        json_file.stem.split(\".\")[0] for json_file in available_json_files\n",
    "    ]\n",
    "\n",
    "    available_videos = []\n",
    "\n",
    "    for video_name in available_video_files:\n",
    "        if video_name.stem.split(\".\")[0] in available_json:\n",
    "            available_videos.append(video_name.name)\n",
    "\n",
    "    return available_videos\n",
    "\n",
    "\n",
    "select_msg = (\n",
    "    \"<style>.widget-label { min-width: 20ex !important; }</style>\"\n",
    "    \"<body><p>Please select the video to explore from the dropdown list. To be available in the list, \"\n",
    "    \"the pose estimation output .json file and the original video file must share the same name, except \"\n",
    "    f\"for the .openpifaf.json extension, and be stored in {source_data_folder}/.</p></body>\"\n",
    ")\n",
    "\n",
    "display(HTML(select_msg))\n",
    "\n",
    "video_selector = Dropdown(\n",
    "    options=get_available_videos(source_data_folder),\n",
    "    description=\"Video to explore:\",\n",
    "    disabled=False,\n",
    "    layout=Layout(width=\"60%\", height=\"40px\"),\n",
    ")\n",
    "\n",
    "video_selector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect video and per-frame pose metadata for the selected video\n",
    "The video should be selected from the drop-down menu above after running the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = f\"{source_data_folder}/{video_selector.value}\"\n",
    "\n",
    "pose_file = f\"{source_data_folder}/{video_selector.value}.openpifpaf.json\"\n",
    "\n",
    "print(\"Video file:\", video_file)\n",
    "print(\"Posedata file:\", pose_file)\n",
    "\n",
    "video_name = \".\".join(video_file.split(\".\")[:-1])\n",
    "\n",
    "cap = cv2.VideoCapture(video_file)\n",
    "video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(\"Video FPS:\", video_fps)\n",
    "\n",
    "print(\"Processing video and JSON files, please wait...\")\n",
    "\n",
    "pose_json = jsonlines.open(pose_file)\n",
    "pose_data = []\n",
    "\n",
    "# Per-frame pose data: frame, seconds, num_poses, avg_pose_conf, avg_coords_per_pose\n",
    "pose_series = {\n",
    "    \"frame\": [],\n",
    "    \"seconds\": [],\n",
    "    \"timestamp\": [],\n",
    "    \"num_poses\": [],\n",
    "    \"avg_score\": [],\n",
    "    \"avg_coords_per_pose\": [],\n",
    "}\n",
    "\n",
    "for frame in pose_json:\n",
    "\n",
    "    pose_data.append(frame)\n",
    "\n",
    "    # Frame output is numbered from 1 in the JSON\n",
    "    seconds = float(frame[\"frame\"] - 1) / video_fps\n",
    "\n",
    "    num_poses = len(frame[\"predictions\"])\n",
    "    pose_series[\"num_poses\"].append(num_poses)\n",
    "\n",
    "    pose_series[\"frame\"].append(frame[\"frame\"] - 1)\n",
    "    pose_series[\"seconds\"].append(seconds)\n",
    "\n",
    "    # Construct a timestamp that can be used with Bokeh's DatetimeTickFormatter\n",
    "    td = timedelta(seconds=seconds)\n",
    "    datestring = str(td)\n",
    "    if td.microseconds == 0:\n",
    "        datestring += \".000000\"\n",
    "    dt = datetime.strptime(datestring, \"%H:%M:%S.%f\")\n",
    "\n",
    "    pose_series[\"timestamp\"].append(dt)\n",
    "\n",
    "    pose_scores = []\n",
    "    pose_coords_counts = []\n",
    "    avg_score = 0  # NaN for empty frames?\n",
    "    avg_coords_per_pose = 0\n",
    "\n",
    "    for pose in frame[\"predictions\"]:\n",
    "\n",
    "        # ??? Do something with the bbox? The avg ratio of bbox area to full screen can indicate closeup\n",
    "        # vs. long shot (could also run monoloco to get this kind of info + more)\n",
    "\n",
    "        pose_scores.append(pose[\"score\"])\n",
    "        pose_coords = 0\n",
    "        for i in range(0, len(pose[\"keypoints\"]), 3):\n",
    "            # Mostly ignore the coord confidence value, unless it's 0 (coord not found)\n",
    "            # These seem to be averaged into the full pose \"score\" already\n",
    "            if pose[\"keypoints\"][i + 2] != 0:\n",
    "                pose_coords += 1\n",
    "\n",
    "        # To find the typically small proportion of poses that are complete\n",
    "        # if pose_coords == 17:\n",
    "        #     print(frame['frame'])\n",
    "\n",
    "        pose_coords_counts.append(pose_coords)\n",
    "\n",
    "    if num_poses > 0:\n",
    "        avg_score = sum(pose_scores) / num_poses\n",
    "        avg_coords_per_pose = sum(pose_coords_counts) / num_poses\n",
    "\n",
    "    pose_series[\"avg_score\"].append(avg_score)\n",
    "    pose_series[\"avg_coords_per_pose\"].append(avg_coords_per_pose)\n",
    "\n",
    "print(\"Total frames:\", len(pose_series[\"frame\"]))\n",
    "\n",
    "print(\"Duration:\", pose_series[\"timestamp\"][len(pose_series[\"timestamp\"]) - 1].time())\n",
    "\n",
    "print(\"Please run the next code cell to launch the explorer app.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and launch the explorer app\n",
    "\n",
    "This displays an interactive chart visualization of the attributes of the posedata in the .json output file across the runtime of the video.\n",
    "\n",
    "Clicking anywhere in the chart, moving the slider, or clicking the prev/next buttons will select a frame and draw the poses detected in that frame, with the option of displaying the actual image from the source video as the \"background.\"\n",
    "\n",
    "Please see the cell below the next if you are running this notebook in VS Code. Note also that the Jupyter server must be running on port 8888 for the explorer app to work in Jupyter/JupterLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The body part numberings and armature connectors for the 17-keypoint COCO pose format are defined in\n",
    "# https://github.com/openpifpaf/openpifpaf/blob/main/src/openpifpaf/plugins/coco/constants.py\n",
    "# Note that the body part numbers in the connector (skeleton) definitions begin with 1, for some reason, not 0\n",
    "OPP_COCO_SKELETON = [\n",
    "    (16, 14),\n",
    "    (14, 12),\n",
    "    (17, 15),\n",
    "    (15, 13),\n",
    "    (12, 13),\n",
    "    (6, 12),\n",
    "    (7, 13),\n",
    "    (6, 7),\n",
    "    (6, 8),\n",
    "    (7, 9),\n",
    "    (8, 10),\n",
    "    (9, 11),\n",
    "    (2, 3),\n",
    "    (1, 2),\n",
    "    (1, 3),\n",
    "    (2, 4),\n",
    "    (3, 5),\n",
    "    (4, 6),\n",
    "    (5, 7),\n",
    "]\n",
    "OPP_COCO_COLORS = [\n",
    "    \"orangered\",\n",
    "    \"orange\",\n",
    "    \"blue\",\n",
    "    \"lightblue\",\n",
    "    \"darkgreen\",\n",
    "    \"red\",\n",
    "    \"lightgreen\",\n",
    "    \"pink\",\n",
    "    \"plum\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"saddlebrown\",\n",
    "    \"mediumorchid\",\n",
    "    \"gray\",\n",
    "    \"salmon\",\n",
    "    \"chartreuse\",\n",
    "    \"lightgray\",\n",
    "    \"darkturquoise\",\n",
    "    \"goldenrod\",\n",
    "]\n",
    "\n",
    "UPSCALE = 3  # See draw_frame()\n",
    "\n",
    "# Default dimensions of the output visualizations\n",
    "FIGURE_WIDTH = 950\n",
    "FIGURE_HEIGHT = 500\n",
    "\n",
    "\n",
    "def image_from_video_frame(video_file, frameno):\n",
    "    \"\"\"Grabs the specified frame from the video and converts it into an RGBA array\"\"\"\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    cap.set(1, frameno)\n",
    "    ret, img = cap.read()\n",
    "    rgba_img = cv2.cvtColor(img, cv2.COLOR_RGB2RGBA)\n",
    "    image = np.asarray(rgba_img)\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_frame(frame, bg_img=None):\n",
    "    \"\"\"Draws the poses in the specified frame, superimposing them on the frame image, if provided.\"\"\"\n",
    "\n",
    "    # The only way to get smooth(er) lines in the pose armatures via PIL ImageDraw is to upscale the entire\n",
    "    # image by some factor, draw the lines, then downscale back to the original resolution while applying\n",
    "    # Lanczos resampling, because ImageDraw doesn't do any native anti-aliasing.\n",
    "    if bg_img is None:\n",
    "        bg_img = Image.new(\"RGBA\", (video_width * UPSCALE, video_height * UPSCALE))\n",
    "    else:\n",
    "        bg_img = bg_img.resize((video_width * UPSCALE, video_height * UPSCALE))\n",
    "\n",
    "    drawing = ImageDraw.Draw(bg_img)\n",
    "\n",
    "    for pose_prediction in frame[\"predictions\"]:\n",
    "        pose_coords = np.array_split(\n",
    "            pose_prediction[\"keypoints\"], len(pose_prediction[\"keypoints\"]) / 3\n",
    "        )\n",
    "\n",
    "        for i, seg in enumerate(OPP_COCO_SKELETON):\n",
    "\n",
    "            if pose_coords[seg[0] - 1][2] == 0 or pose_coords[seg[1] - 1][2] == 0:\n",
    "                continue\n",
    "\n",
    "            line_color = OPP_COCO_COLORS[i]\n",
    "            shape = [\n",
    "                (\n",
    "                    int(pose_coords[seg[0] - 1][0] * UPSCALE),\n",
    "                    int(pose_coords[seg[0] - 1][1] * UPSCALE),\n",
    "                ),\n",
    "                (\n",
    "                    int(pose_coords[seg[1] - 1][0] * UPSCALE),\n",
    "                    int(pose_coords[seg[1] - 1][1]) * UPSCALE,\n",
    "                ),\n",
    "            ]\n",
    "            drawing.line(shape, fill=line_color, width=2 * UPSCALE)\n",
    "\n",
    "    bg_img = bg_img.resize(\n",
    "        (video_width, video_height), resample=Image.Resampling.LANCZOS\n",
    "    )\n",
    "\n",
    "    return bg_img\n",
    "\n",
    "\n",
    "def pil_to_bokeh_image(pil_img):\n",
    "    \"\"\"The Bokeh interactive notebook tools will only display image data if it's formatted in a particular way\"\"\"\n",
    "    img_array = np.array(pil_img.transpose(Image.Transpose.FLIP_TOP_BOTTOM))\n",
    "\n",
    "    img = np.empty(img_array.shape[:2], dtype=np.uint32)\n",
    "    view = img.view(dtype=np.uint8).reshape(img_array.shape)\n",
    "\n",
    "    for i in range(video_height):\n",
    "        for j in range(video_width):\n",
    "            view[i, j, 0] = img_array[i, j, 0]\n",
    "            view[i, j, 1] = img_array[i, j, 1]\n",
    "            view[i, j, 2] = img_array[i, j, 2]\n",
    "            view[i, j, 3] = img_array[i, j, 3]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def bkapp(doc):\n",
    "    \"\"\"Define and run the Bokeh interactive notebook (Python + Javascript) application\"\"\"\n",
    "\n",
    "    max_y = max(pose_series[\"avg_coords_per_pose\"] + pose_series[\"num_poses\"])\n",
    "\n",
    "    # This is the main interactive chart figure\n",
    "    p = figure(\n",
    "        width=FIGURE_WIDTH,\n",
    "        height=FIGURE_HEIGHT,\n",
    "        title=video_name,\n",
    "        min_border=10,\n",
    "        y_range=(0, max_y + 1),\n",
    "        tools=\"save,box_zoom,pan,reset\",\n",
    "    )\n",
    "    # Format the X axis as hour-minute-second timecodes\n",
    "    p.x_range = Range1d(min(pose_series[\"timestamp\"]), max(pose_series[\"timestamp\"]))\n",
    "    p.xaxis.axis_label = \"Time\"\n",
    "    time_formatter = DatetimeTickFormatter(\n",
    "        hourmin=\"%H:%M:%S\",\n",
    "        minutes=\"%H:%M:%S\",\n",
    "        minsec=\"%H:%M:%S\",\n",
    "        seconds=\"%Ss\",\n",
    "        milliseconds=\"%3Nms\",\n",
    "    )\n",
    "    p.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"num_poses\"],\n",
    "        legend_label=\"Poses per frame\",\n",
    "        color=\"blue\",\n",
    "        alpha=0.6,\n",
    "        line_width=2,\n",
    "    )\n",
    "    p.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"avg_coords_per_pose\"],\n",
    "        legend_label=\"Coords per pose\",\n",
    "        color=\"red\",\n",
    "        alpha=0.6,\n",
    "        line_width=2,\n",
    "    )\n",
    "    # The left Y axis corresponds to counts of poses and coordinates\n",
    "    p.yaxis.axis_label = \"Poses or Coords\"\n",
    "    p.extra_y_ranges = {\"avg_score\": Range1d(0, 1)}\n",
    "    p.line(\n",
    "        pose_series[\"timestamp\"],\n",
    "        pose_series[\"avg_score\"],\n",
    "        y_range_name=\"avg_score\",\n",
    "        legend_label=\"Avg pose score\",\n",
    "        color=\"green\",\n",
    "        alpha=0.4,\n",
    "        line_width=2,\n",
    "    )\n",
    "    # The right Y axis corresponds to the average pose score (from 0 to 1)\n",
    "    p.add_layout(\n",
    "        LinearAxis(y_range_name=\"avg_score\", axis_label=\"Avg Pose Score\"), \"right\"\n",
    "    )\n",
    "    p.xaxis.formatter = time_formatter\n",
    "    p.xaxis.ticker.desired_num_ticks = 10\n",
    "    p.legend.click_policy = \"hide\"\n",
    "\n",
    "    def tap_callback(event):\n",
    "        \"\"\"When the chart is clicked, move the slider to the appropriate frame\"\"\"\n",
    "        # event.x is a timestamp, so it needs to be converted to a frameno\n",
    "        start_dt = datetime(1900, 1, 1)\n",
    "        dt = datetime.utcfromtimestamp(event.x / 1000)\n",
    "        t_delta = dt - start_dt\n",
    "        clicked_frame = round(t_delta.total_seconds() * video_fps)\n",
    "        slider.value = clicked_frame\n",
    "\n",
    "    tap = TapTool()\n",
    "    crosshair = CrosshairTool()\n",
    "\n",
    "    def get_frame_info(fn):\n",
    "        return f\"Frame info: {pose_series['num_poses'][fn]} detected poses, {pose_series['avg_coords_per_pose'][fn]} avg coords/pose, {pose_series['avg_score'][fn]} avg pose score\"\n",
    "\n",
    "    info_div = Div(text=get_frame_info(0))\n",
    "\n",
    "    p.add_tools(tap, crosshair)\n",
    "    p.on_event(\"tap\", tap_callback)\n",
    "\n",
    "    # This is the second figure, where the poses in the selected frame are drawn\n",
    "    frame_p = figure(\n",
    "        x_range=(0, video_width),\n",
    "        y_range=(0, video_height),\n",
    "        width=FIGURE_WIDTH,\n",
    "        height=int(FIGURE_WIDTH / video_width * video_height),\n",
    "        title=\"Poses in selected frame\",\n",
    "        tools=\"save\",\n",
    "    )\n",
    "    # Add an invisible glyph to suppress the \"figure has no renderers\" warning\n",
    "    frame_p.circle(0, 0, size=0, alpha=0.0)\n",
    "\n",
    "    def background_toggle_handler(event):\n",
    "        \"\"\"When the image underlay is toggled on or off, prompt the slider to redraw the frame\"\"\"\n",
    "        slider_callback(None, slider.value, slider.value)\n",
    "\n",
    "    background_switch = Toggle(label=\"show background\", active=False)\n",
    "    background_switch.on_click(background_toggle_handler)\n",
    "\n",
    "    def slider_callback(attr, old, new):\n",
    "        \"\"\"When the slider moves, draw the poses in the new frame and show the background if desired\"\"\"\n",
    "        frame_p.renderers = []\n",
    "        if background_switch.active:\n",
    "            cap = cv2.VideoCapture(video_file)\n",
    "            cap.set(1, new)\n",
    "            ret, img = cap.read()\n",
    "            rgb_bg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            rgba_bg = cv2.cvtColor(rgb_bg, cv2.COLOR_RGB2RGBA)\n",
    "            pil_bg = Image.fromarray(rgba_bg)\n",
    "            img = draw_frame(pose_data[new], pil_bg)\n",
    "        else:\n",
    "            img = draw_frame(pose_data[new])\n",
    "        img = pil_to_bokeh_image(img)\n",
    "        frame_p.image_rgba(image=[img], x=0, y=0, dw=img.shape[1], dh=img.shape[0])\n",
    "        info_div.text = get_frame_info(new)\n",
    "\n",
    "    slider = Slider(\n",
    "        start=0, end=len(pose_data) - 1, value=0, step=1, title=\"Selected frame\"\n",
    "    )\n",
    "    slider.on_change(\"value\", slider_callback)\n",
    "\n",
    "    def prev_handler(event):\n",
    "        slider.value = max(0, slider.value - 1)\n",
    "\n",
    "    def next_handler(event):\n",
    "        slider.value = min(slider.value + 1, len(pose_data) - 1)\n",
    "\n",
    "    prev_button = Button(label=\"prev\")\n",
    "    prev_button.on_click(prev_handler)\n",
    "    next_button = Button(label=\"next\")\n",
    "    next_button.on_click(next_handler)\n",
    "\n",
    "    control_row = row(children=[prev_button, next_button, background_switch])\n",
    "\n",
    "    layout_column = column(p, slider, info_div, control_row, frame_p)\n",
    "    \n",
    "    doc.add_root(layout_column)\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "show(bkapp, notebook_url=\"localhost:8888\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the notebook in VS Code:** As of late 2022, if you are running this notebook in VS Code instead of Jupyter or JupyterLab, the above cell will not work (BokehJS will load, but no figures will appear) without the following workaround:\n",
    "\n",
    "Take note of the error message that appears when you try to run the cell above, particularly the long alphanumeric string suggested as a value for `BOKEH_ALLOW_WS_ORIGIN`. Copy this string, then uncomment the last two lines in the cell below, paste the alphanumeric string in place of the `INSERT_BOKEH_ALLOW_WS_ORIGIN_VALUE_HERE` text, run the cell, then try running the cell above to launch the explorer app again. It should work now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are following the steps above to run the explorer app in VS Code,\n",
    "# uncomment the following lines (remove the '#'s) before running this cell:\n",
    "#import os\n",
    "#os.environ[\"BOKEH_ALLOW_WS_ORIGIN\"] = \"INSERT_BOKEH_ALLOW_WS_ORIGIN_VALUE_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of frame-by-frame pose drawing\n",
    "\n",
    "The cell below uses a different viz library to draw the poses in each successive frame on an HTML canvas, at the same frame rate as the source video.\n",
    "\n",
    "Note that this drawing library (`ipycanvas`) doesn't play well with the Bokeh interactive application above, which is why the somewhat clunkier PIL ImageDraw library is used to draw the poses there instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "canvas = Canvas(width=video_width, height=video_height, sync_image_data=True)\n",
    "\n",
    "display(canvas)\n",
    "\n",
    "\n",
    "def draw_frame_on_canvas(frame, canvas):\n",
    "\n",
    "    for pose_prediction in frame[\"predictions\"]:\n",
    "        pose_coords = np.array_split(\n",
    "            pose_prediction[\"keypoints\"], len(pose_prediction[\"keypoints\"]) / 3\n",
    "        )\n",
    "\n",
    "        for i, seg in enumerate(OPP_COCO_SKELETON):\n",
    "\n",
    "            if pose_coords[seg[0] - 1][2] == 0 or pose_coords[seg[1] - 1][2] == 0:\n",
    "                continue\n",
    "\n",
    "            canvas.stroke_style = OPP_COCO_COLORS[i]\n",
    "            canvas.line_width = 2\n",
    "\n",
    "            canvas.stroke_line(\n",
    "                pose_coords[seg[0] - 1][0],\n",
    "                pose_coords[seg[0] - 1][1],\n",
    "                pose_coords[seg[1] - 1][0],\n",
    "                pose_coords[seg[1] - 1][1],\n",
    "            )\n",
    "\n",
    "\n",
    "# This will \"animate\" all of the detected poses starting from the beginning of the video\n",
    "for frame in pose_data:\n",
    "\n",
    "    with hold_canvas():\n",
    "\n",
    "        canvas.clear()\n",
    "\n",
    "        draw_frame_on_canvas(frame, canvas)\n",
    "\n",
    "        sleep(1 / video_fps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
